{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1REASsY8RBAGg57wAeZAMm_VqXSoAPhbT",
      "authorship_tag": "ABX9TyNchmHZ/nWZxH+uYvFS06mz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/BERT/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMizlRlP5SyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "3a5a9365-54be-4ee2-8af0-19e463305de8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 8.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 58.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=8e2c1601a746aae5eb296783d16ff681f28e1e1a688b997f9c0b931692506758\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHywV9Kp5abi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/My\\ Drive/Hinglish/clean_data/train_lm_data.txt .\n",
        "!cp drive/My\\ Drive/Hinglish/clean_data/dev_lm_data.txt .\n",
        "!cp drive/My\\ Drive/Hinglish/clean_data/test_lm_data.txt .\n",
        "# !cp drive/My\\ Drive/run_language_modeling.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P1L6UW9ZUcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ae7b700b-0cb7-40bb-ca87-c7fc3042a7ba"
      },
      "source": [
        "!head train_lm_data.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Wah kali :winking_face:\n",
            "\n",
            "  Is jesay paglo ko ahsas nahi is jesi orat nau hi isay peda kiya agar is ki maa dekh lay k… \n",
            "\n",
            "Honourable home minister Shri Amit Shah ji Ko unke janamdiwas ki dher saari badhai aur shubhkamnay .Congress mukt B… \n",
            "\n",
            " Tshini and ke ndiyayaz ke imoto yakho iyatyisa :face_with_tears_of_joy:\n",
            "\n",
            " unnecessarily instigate kar rahi thi ko\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pFzOxkJ5bvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b4ea3d9-d75a-4321-bcbe-010159f398a2"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=distilBert6 --model_type=distilbert --model_name_or_path=drive/My\\ Drive/distilBert6 --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 5 --save_total_limit 2 --overwrite_output_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/26/2020 21:30:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/26/2020 21:30:36 - INFO - transformers.configuration_utils -   loading configuration file drive/My Drive/distilBert6/config.json\n",
            "02/26/2020 21:30:36 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"dim\": 768,\n",
            "  \"do_sample\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"temperature\": 1.0,\n",
            "  \"tie_weights_\": true,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   Model name 'drive/My Drive/distilBert6' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming 'drive/My Drive/distilBert6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   Didn't find file drive/My Drive/distilBert6/added_tokens.json. We won't load it.\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/distilBert6/vocab.txt\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/distilBert6/special_tokens_map.json\n",
            "02/26/2020 21:30:36 - INFO - transformers.tokenization_utils -   loading file drive/My Drive/distilBert6/tokenizer_config.json\n",
            "02/26/2020 21:30:36 - INFO - transformers.modeling_utils -   loading weights file drive/My Drive/distilBert6/pytorch_model.bin\n",
            "02/26/2020 21:30:41 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='dev_lm_data.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='drive/My Drive/distilBert6', model_type='distilbert', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='distilBert6', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=2, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_lm_data.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/26/2020 21:30:41 - INFO - __main__ -   Creating features from dataset file at \n",
            "02/26/2020 21:32:36 - INFO - __main__ -   Saving features into cached file distilbert_cached_lm_510_train_lm_data.txt\n",
            "02/26/2020 21:32:37 - INFO - __main__ -   ***** Running training *****\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Num examples = 31333\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Num Epochs = 5\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Total optimization steps = 39170\n",
            "02/26/2020 21:32:37 - INFO - __main__ -     Starting fine-tuning.\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/7834 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/7834 [00:00<30:55,  4.22it/s]\u001b[A\n",
            "Iteration:   0% 2/7834 [00:00<28:51,  4.52it/s]\u001b[A\n",
            "Iteration:   0% 3/7834 [00:00<27:06,  4.81it/s]\u001b[A\n",
            "Iteration:   0% 4/7834 [00:00<25:45,  5.07it/s]\u001b[A\n",
            "Iteration:   0% 5/7834 [00:00<24:55,  5.23it/s]\u001b[A\n",
            "Iteration:   0% 6/7834 [00:01<24:14,  5.38it/s]\u001b[A\n",
            "Iteration:   0% 7/7834 [00:01<23:48,  5.48it/s]\u001b[A\n",
            "Iteration:   0% 8/7834 [00:01<23:30,  5.55it/s]\u001b[A\n",
            "Iteration:   0% 9/7834 [00:01<23:13,  5.62it/s]\u001b[A\n",
            "Iteration:   0% 10/7834 [00:01<23:02,  5.66it/s]\u001b[A\n",
            "Iteration:   0% 11/7834 [00:01<22:53,  5.70it/s]\u001b[A\n",
            "Iteration:   0% 12/7834 [00:02<22:47,  5.72it/s]\u001b[A\n",
            "Iteration:   0% 13/7834 [00:02<22:42,  5.74it/s]\u001b[A\n",
            "Iteration:   0% 14/7834 [00:02<22:44,  5.73it/s]\u001b[A\n",
            "Iteration:   0% 15/7834 [00:02<22:42,  5.74it/s]\u001b[A\n",
            "Iteration:   0% 16/7834 [00:02<22:39,  5.75it/s]\u001b[A\n",
            "Iteration:   0% 17/7834 [00:03<22:50,  5.70it/s]\u001b[A\n",
            "Iteration:   0% 18/7834 [00:03<22:45,  5.73it/s]\u001b[A\n",
            "Iteration:   0% 19/7834 [00:03<22:54,  5.69it/s]\u001b[A\n",
            "Iteration:   0% 20/7834 [00:03<23:04,  5.64it/s]\u001b[A\n",
            "Iteration:   0% 21/7834 [00:03<23:19,  5.58it/s]\u001b[A\n",
            "Iteration:   0% 22/7834 [00:03<23:23,  5.57it/s]\u001b[A\n",
            "Iteration:   0% 23/7834 [00:04<23:29,  5.54it/s]\u001b[A\n",
            "Iteration:   0% 24/7834 [00:04<23:32,  5.53it/s]\u001b[A\n",
            "Iteration:   0% 25/7834 [00:04<23:17,  5.59it/s]\u001b[A\n",
            "Iteration:   0% 26/7834 [00:04<23:02,  5.65it/s]\u001b[A\n",
            "Iteration:   0% 27/7834 [00:04<22:56,  5.67it/s]\u001b[A\n",
            "Iteration:   0% 28/7834 [00:04<22:53,  5.68it/s]\u001b[A\n",
            "Iteration:   0% 29/7834 [00:05<23:03,  5.64it/s]\u001b[A\n",
            "Iteration:   0% 30/7834 [00:05<22:55,  5.67it/s]\u001b[A\n",
            "Iteration:   0% 31/7834 [00:05<22:46,  5.71it/s]\u001b[A\n",
            "Iteration:   0% 32/7834 [00:05<22:41,  5.73it/s]\u001b[A\n",
            "Iteration:   0% 33/7834 [00:05<22:44,  5.72it/s]\u001b[A\n",
            "Iteration:   0% 34/7834 [00:06<22:46,  5.71it/s]\u001b[A\n",
            "Iteration:   0% 35/7834 [00:06<22:48,  5.70it/s]\u001b[A\n",
            "Iteration:   0% 36/7834 [00:06<22:42,  5.72it/s]\u001b[A\n",
            "Iteration:   0% 37/7834 [00:06<22:40,  5.73it/s]\u001b[A\n",
            "Iteration:   0% 38/7834 [00:06<22:38,  5.74it/s]\u001b[A\n",
            "Iteration:   0% 39/7834 [00:06<22:41,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 40/7834 [00:07<22:38,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 41/7834 [00:07<22:37,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 42/7834 [00:07<22:37,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 43/7834 [00:07<22:36,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 44/7834 [00:07<22:52,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 45/7834 [00:07<22:43,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 46/7834 [00:08<22:39,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 47/7834 [00:08<22:38,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 48/7834 [00:08<22:36,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 49/7834 [00:08<22:49,  5.69it/s]\u001b[A\n",
            "Iteration:   1% 50/7834 [00:08<22:47,  5.69it/s]\u001b[A\n",
            "Iteration:   1% 51/7834 [00:09<22:49,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 52/7834 [00:09<22:55,  5.66it/s]\u001b[A\n",
            "Iteration:   1% 53/7834 [00:09<22:48,  5.69it/s]\u001b[A\n",
            "Iteration:   1% 54/7834 [00:09<22:45,  5.70it/s]\u001b[A\n",
            "Iteration:   1% 55/7834 [00:09<22:43,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 56/7834 [00:09<22:39,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 57/7834 [00:10<22:53,  5.66it/s]\u001b[A\n",
            "Iteration:   1% 58/7834 [00:10<22:45,  5.70it/s]\u001b[A\n",
            "Iteration:   1% 59/7834 [00:10<22:42,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 60/7834 [00:10<22:38,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 61/7834 [00:10<22:35,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 62/7834 [00:10<22:33,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 63/7834 [00:11<22:31,  5.75it/s]\u001b[A\n",
            "Iteration:   1% 64/7834 [00:11<22:31,  5.75it/s]\u001b[A\n",
            "Iteration:   1% 65/7834 [00:11<22:29,  5.76it/s]\u001b[A\n",
            "Iteration:   1% 66/7834 [00:11<22:37,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 67/7834 [00:11<22:35,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 68/7834 [00:11<22:41,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 69/7834 [00:12<22:36,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 70/7834 [00:12<22:37,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 71/7834 [00:12<22:33,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 72/7834 [00:12<22:33,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 73/7834 [00:12<22:45,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 74/7834 [00:13<22:39,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 75/7834 [00:13<22:40,  5.70it/s]\u001b[A\n",
            "Iteration:   1% 76/7834 [00:13<22:34,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 77/7834 [00:13<22:53,  5.65it/s]\u001b[A\n",
            "Iteration:   1% 78/7834 [00:13<22:46,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 79/7834 [00:13<22:45,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 80/7834 [00:14<22:38,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 81/7834 [00:14<22:34,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 82/7834 [00:14<22:38,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 83/7834 [00:14<22:44,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 84/7834 [00:14<22:39,  5.70it/s]\u001b[A\n",
            "Iteration:   1% 85/7834 [00:14<22:42,  5.69it/s]\u001b[A\n",
            "Iteration:   1% 86/7834 [00:15<22:35,  5.72it/s]\u001b[A\n",
            "Iteration:   1% 87/7834 [00:15<22:31,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 88/7834 [00:15<22:30,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 89/7834 [00:15<22:25,  5.76it/s]\u001b[A\n",
            "Iteration:   1% 90/7834 [00:15<22:25,  5.75it/s]\u001b[A\n",
            "Iteration:   1% 91/7834 [00:16<22:25,  5.76it/s]\u001b[A\n",
            "Iteration:   1% 92/7834 [00:16<22:31,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 93/7834 [00:16<22:35,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 94/7834 [00:16<22:29,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 95/7834 [00:16<22:25,  5.75it/s]\u001b[A\n",
            "Iteration:   1% 96/7834 [00:16<22:23,  5.76it/s]\u001b[A\n",
            "Iteration:   1% 97/7834 [00:17<22:45,  5.66it/s]\u001b[A\n",
            "Iteration:   1% 98/7834 [00:17<22:49,  5.65it/s]\u001b[A\n",
            "Iteration:   1% 99/7834 [00:17<22:49,  5.65it/s]\u001b[A\n",
            "Iteration:   1% 100/7834 [00:17<22:46,  5.66it/s]\u001b[A\n",
            "Iteration:   1% 101/7834 [00:17<22:50,  5.64it/s]\u001b[A\n",
            "Iteration:   1% 102/7834 [00:17<22:40,  5.68it/s]\u001b[A\n",
            "Iteration:   1% 103/7834 [00:18<22:33,  5.71it/s]\u001b[A\n",
            "Iteration:   1% 104/7834 [00:18<22:29,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 105/7834 [00:18<22:28,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 106/7834 [00:18<22:27,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 107/7834 [00:18<22:23,  5.75it/s]\u001b[A\n",
            "Iteration:   1% 108/7834 [00:18<22:20,  5.76it/s]\u001b[A\n",
            "Iteration:   1% 109/7834 [00:19<22:17,  5.77it/s]\u001b[A\n",
            "Iteration:   1% 110/7834 [00:19<22:16,  5.78it/s]\u001b[A\n",
            "Iteration:   1% 111/7834 [00:19<22:15,  5.78it/s]\u001b[A\n",
            "Iteration:   1% 112/7834 [00:19<22:14,  5.79it/s]\u001b[A\n",
            "Iteration:   1% 113/7834 [00:19<22:14,  5.79it/s]\u001b[A\n",
            "Iteration:   1% 114/7834 [00:20<22:14,  5.79it/s]\u001b[A\n",
            "Iteration:   1% 115/7834 [00:20<22:27,  5.73it/s]\u001b[A\n",
            "Iteration:   1% 116/7834 [00:20<22:24,  5.74it/s]\u001b[A\n",
            "Iteration:   1% 117/7834 [00:20<22:21,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 118/7834 [00:20<22:19,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 119/7834 [00:20<22:25,  5.74it/s]\u001b[A\n",
            "Iteration:   2% 120/7834 [00:21<22:22,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 121/7834 [00:21<22:21,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 122/7834 [00:21<22:30,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 123/7834 [00:21<22:28,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 124/7834 [00:21<22:27,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 125/7834 [00:21<22:25,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 126/7834 [00:22<22:35,  5.69it/s]\u001b[A\n",
            "Iteration:   2% 127/7834 [00:22<22:33,  5.70it/s]\u001b[A\n",
            "Iteration:   2% 128/7834 [00:22<22:27,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 129/7834 [00:22<22:30,  5.70it/s]\u001b[A\n",
            "Iteration:   2% 130/7834 [00:22<22:30,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 131/7834 [00:23<22:23,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 132/7834 [00:23<22:20,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 133/7834 [00:23<22:23,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 134/7834 [00:23<22:23,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 135/7834 [00:23<22:26,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 136/7834 [00:23<22:24,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 137/7834 [00:24<22:20,  5.74it/s]\u001b[A\n",
            "Iteration:   2% 138/7834 [00:24<22:19,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 139/7834 [00:24<22:36,  5.67it/s]\u001b[A\n",
            "Iteration:   2% 140/7834 [00:24<22:45,  5.64it/s]\u001b[A\n",
            "Iteration:   2% 141/7834 [00:24<22:40,  5.66it/s]\u001b[A\n",
            "Iteration:   2% 142/7834 [00:24<22:35,  5.67it/s]\u001b[A\n",
            "Iteration:   2% 143/7834 [00:25<22:26,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 144/7834 [00:25<22:21,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 145/7834 [00:25<22:30,  5.69it/s]\u001b[A\n",
            "Iteration:   2% 146/7834 [00:25<22:30,  5.69it/s]\u001b[A\n",
            "Iteration:   2% 147/7834 [00:25<22:24,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 148/7834 [00:25<22:20,  5.74it/s]\u001b[A\n",
            "Iteration:   2% 149/7834 [00:26<22:16,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 150/7834 [00:26<22:23,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 151/7834 [00:26<22:27,  5.70it/s]\u001b[A\n",
            "Iteration:   2% 152/7834 [00:26<22:25,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 153/7834 [00:26<22:20,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 154/7834 [00:27<22:18,  5.74it/s]\u001b[A\n",
            "Iteration:   2% 155/7834 [00:27<22:16,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 156/7834 [00:27<22:12,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 157/7834 [00:27<22:23,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 158/7834 [00:27<22:34,  5.67it/s]\u001b[A\n",
            "Iteration:   2% 159/7834 [00:27<22:27,  5.69it/s]\u001b[A\n",
            "Iteration:   2% 160/7834 [00:28<22:22,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 161/7834 [00:28<22:20,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 162/7834 [00:28<22:17,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 163/7834 [00:28<22:33,  5.67it/s]\u001b[A\n",
            "Iteration:   2% 164/7834 [00:28<22:27,  5.69it/s]\u001b[A\n",
            "Iteration:   2% 165/7834 [00:28<22:21,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 166/7834 [00:29<22:25,  5.70it/s]\u001b[A\n",
            "Iteration:   2% 167/7834 [00:29<22:22,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 168/7834 [00:29<22:22,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 169/7834 [00:29<22:20,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 170/7834 [00:29<22:18,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 171/7834 [00:30<22:17,  5.73it/s]\u001b[A\n",
            "Iteration:   2% 172/7834 [00:30<22:13,  5.74it/s]\u001b[A\n",
            "Iteration:   2% 173/7834 [00:30<22:10,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 174/7834 [00:30<22:10,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 175/7834 [00:30<22:09,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 176/7834 [00:30<22:12,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 177/7834 [00:31<22:11,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 178/7834 [00:31<22:07,  5.77it/s]\u001b[A\n",
            "Iteration:   2% 179/7834 [00:31<22:06,  5.77it/s]\u001b[A\n",
            "Iteration:   2% 180/7834 [00:31<22:10,  5.75it/s]\u001b[A\n",
            "Iteration:   2% 181/7834 [00:31<22:08,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 182/7834 [00:31<22:07,  5.76it/s]\u001b[A\n",
            "Iteration:   2% 183/7834 [00:32<22:05,  5.77it/s]\u001b[A\n",
            "Iteration:   2% 184/7834 [00:32<22:03,  5.78it/s]\u001b[A\n",
            "Iteration:   2% 185/7834 [00:32<22:05,  5.77it/s]\u001b[A\n",
            "Iteration:   2% 186/7834 [00:32<22:06,  5.77it/s]\u001b[A\n",
            "Iteration:   2% 187/7834 [00:32<22:16,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 188/7834 [00:32<22:15,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 189/7834 [00:33<22:16,  5.72it/s]\u001b[A\n",
            "Iteration:   2% 190/7834 [00:33<22:18,  5.71it/s]\u001b[A\n",
            "Iteration:   2% 191/7834 [00:33<22:20,  5.70it/s]\u001b[A\n",
            "Iteration:   2% 192/7834 [00:33<22:29,  5.66it/s]\u001b[A\n",
            "Iteration:   2% 193/7834 [00:33<22:45,  5.60it/s]\u001b[A\n",
            "Iteration:   2% 194/7834 [00:34<22:51,  5.57it/s]\u001b[A\n",
            "Iteration:   2% 195/7834 [00:34<22:40,  5.61it/s]\u001b[A\n",
            "Iteration:   3% 196/7834 [00:34<22:42,  5.61it/s]\u001b[A\n",
            "Iteration:   3% 197/7834 [00:34<22:49,  5.58it/s]\u001b[A\n",
            "Iteration:   3% 198/7834 [00:34<22:48,  5.58it/s]\u001b[A\n",
            "Iteration:   3% 199/7834 [00:34<23:03,  5.52it/s]\u001b[A\n",
            "Iteration:   3% 200/7834 [00:35<22:57,  5.54it/s]\u001b[A\n",
            "Iteration:   3% 201/7834 [00:35<22:58,  5.54it/s]\u001b[A\n",
            "Iteration:   3% 202/7834 [00:35<22:58,  5.54it/s]\u001b[A\n",
            "Iteration:   3% 203/7834 [00:35<22:54,  5.55it/s]\u001b[A\n",
            "Iteration:   3% 204/7834 [00:35<22:44,  5.59it/s]\u001b[A\n",
            "Iteration:   3% 205/7834 [00:35<22:29,  5.65it/s]\u001b[A\n",
            "Iteration:   3% 206/7834 [00:36<22:27,  5.66it/s]\u001b[A\n",
            "Iteration:   3% 207/7834 [00:36<22:32,  5.64it/s]\u001b[A\n",
            "Iteration:   3% 208/7834 [00:36<22:38,  5.61it/s]\u001b[A\n",
            "Iteration:   3% 209/7834 [00:36<22:44,  5.59it/s]\u001b[A\n",
            "Iteration:   3% 210/7834 [00:36<22:58,  5.53it/s]\u001b[A\n",
            "Iteration:   3% 211/7834 [00:37<22:56,  5.54it/s]\u001b[A\n",
            "Iteration:   3% 212/7834 [00:37<22:57,  5.53it/s]\u001b[A\n",
            "Iteration:   3% 213/7834 [00:37<22:49,  5.56it/s]\u001b[A\n",
            "Iteration:   3% 214/7834 [00:37<22:36,  5.62it/s]\u001b[A\n",
            "Iteration:   3% 215/7834 [00:37<22:25,  5.66it/s]\u001b[A\n",
            "Iteration:   3% 216/7834 [00:37<22:23,  5.67it/s]\u001b[A\n",
            "Iteration:   3% 217/7834 [00:38<22:16,  5.70it/s]\u001b[A\n",
            "Iteration:   3% 218/7834 [00:38<22:15,  5.70it/s]\u001b[A\n",
            "Iteration:   3% 219/7834 [00:38<22:15,  5.70it/s]\u001b[A\n",
            "Iteration:   3% 220/7834 [00:38<22:20,  5.68it/s]\u001b[A\n",
            "Iteration:   3% 221/7834 [00:38<22:13,  5.71it/s]\u001b[A\n",
            "Iteration:   3% 222/7834 [00:39<22:09,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 223/7834 [00:39<22:05,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 224/7834 [00:39<22:09,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 225/7834 [00:39<22:06,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 226/7834 [00:39<22:04,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 227/7834 [00:39<22:03,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 228/7834 [00:40<22:01,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 229/7834 [00:40<22:08,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 230/7834 [00:40<22:05,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 231/7834 [00:40<22:04,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 232/7834 [00:40<22:00,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 233/7834 [00:40<22:01,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 234/7834 [00:41<21:59,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 235/7834 [00:41<21:58,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 236/7834 [00:41<21:56,  5.77it/s]\u001b[A\n",
            "Iteration:   3% 237/7834 [00:41<21:55,  5.78it/s]\u001b[A\n",
            "Iteration:   3% 238/7834 [00:41<21:54,  5.78it/s]\u001b[A\n",
            "Iteration:   3% 239/7834 [00:41<21:53,  5.78it/s]\u001b[A\n",
            "Iteration:   3% 240/7834 [00:42<21:52,  5.78it/s]\u001b[A\n",
            "Iteration:   3% 241/7834 [00:42<21:55,  5.77it/s]\u001b[A\n",
            "Iteration:   3% 242/7834 [00:42<22:06,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 243/7834 [00:42<22:02,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 244/7834 [00:42<21:59,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 245/7834 [00:43<21:57,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 246/7834 [00:43<21:56,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 247/7834 [00:43<21:55,  5.77it/s]\u001b[A\n",
            "Iteration:   3% 248/7834 [00:43<21:53,  5.77it/s]\u001b[A\n",
            "Iteration:   3% 249/7834 [00:43<21:55,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 250/7834 [00:43<22:06,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 251/7834 [00:44<22:05,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 252/7834 [00:44<22:02,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 253/7834 [00:44<21:58,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 254/7834 [00:44<21:58,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 255/7834 [00:44<21:56,  5.76it/s]\u001b[A\n",
            "Iteration:   3% 256/7834 [00:44<22:05,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 257/7834 [00:45<22:02,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 258/7834 [00:45<22:00,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 259/7834 [00:45<21:58,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 260/7834 [00:45<22:03,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 261/7834 [00:45<21:57,  5.75it/s]\u001b[A\n",
            "Iteration:   3% 262/7834 [00:45<22:11,  5.69it/s]\u001b[A\n",
            "Iteration:   3% 263/7834 [00:46<22:04,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 264/7834 [00:46<22:00,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 265/7834 [00:46<21:57,  5.74it/s]\u001b[A\n",
            "Iteration:   3% 266/7834 [00:46<22:02,  5.72it/s]\u001b[A\n",
            "Iteration:   3% 267/7834 [00:46<22:01,  5.73it/s]\u001b[A\n",
            "Iteration:   3% 268/7834 [00:47<22:05,  5.71it/s]\u001b[A\n",
            "Iteration:   3% 269/7834 [00:47<22:09,  5.69it/s]\u001b[A\n",
            "Iteration:   3% 270/7834 [00:47<22:12,  5.68it/s]\u001b[A\n",
            "Iteration:   3% 271/7834 [00:47<22:10,  5.69it/s]\u001b[A\n",
            "Iteration:   3% 272/7834 [00:47<22:17,  5.66it/s]\u001b[A\n",
            "Iteration:   3% 273/7834 [00:47<22:23,  5.63it/s]\u001b[A\n",
            "Iteration:   3% 274/7834 [00:48<22:16,  5.66it/s]\u001b[A\n",
            "Iteration:   4% 275/7834 [00:48<22:10,  5.68it/s]\u001b[A\n",
            "Iteration:   4% 276/7834 [00:48<22:04,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 277/7834 [00:48<22:01,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 278/7834 [00:48<21:57,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 279/7834 [00:48<21:56,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 280/7834 [00:49<21:58,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 281/7834 [00:49<21:58,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 282/7834 [00:49<21:57,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 283/7834 [00:49<21:54,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 284/7834 [00:49<21:52,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 285/7834 [00:49<21:48,  5.77it/s]\u001b[A\n",
            "Iteration:   4% 286/7834 [00:50<21:48,  5.77it/s]\u001b[A\n",
            "Iteration:   4% 287/7834 [00:50<21:49,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 288/7834 [00:50<21:49,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 289/7834 [00:50<21:49,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 290/7834 [00:50<21:49,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 291/7834 [00:51<21:52,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 292/7834 [00:51<21:50,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 293/7834 [00:51<21:51,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 294/7834 [00:51<21:57,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 295/7834 [00:51<22:01,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 296/7834 [00:51<21:56,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 297/7834 [00:52<22:05,  5.69it/s]\u001b[A\n",
            "Iteration:   4% 298/7834 [00:52<22:13,  5.65it/s]\u001b[A\n",
            "Iteration:   4% 299/7834 [00:52<22:15,  5.64it/s]\u001b[A\n",
            "Iteration:   4% 300/7834 [00:52<22:10,  5.66it/s]\u001b[A\n",
            "Iteration:   4% 301/7834 [00:52<22:01,  5.70it/s]\u001b[A\n",
            "Iteration:   4% 302/7834 [00:52<21:58,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 303/7834 [00:53<22:13,  5.65it/s]\u001b[A\n",
            "Iteration:   4% 304/7834 [00:53<22:05,  5.68it/s]\u001b[A\n",
            "Iteration:   4% 305/7834 [00:53<21:58,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 306/7834 [00:53<21:55,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 307/7834 [00:53<21:53,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 308/7834 [00:54<21:50,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 309/7834 [00:54<21:49,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 310/7834 [00:54<22:02,  5.69it/s]\u001b[A\n",
            "Iteration:   4% 311/7834 [00:54<22:00,  5.70it/s]\u001b[A\n",
            "Iteration:   4% 312/7834 [00:54<21:56,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 313/7834 [00:54<21:54,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 314/7834 [00:55<21:53,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 315/7834 [00:55<21:49,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 316/7834 [00:55<21:49,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 317/7834 [00:55<22:13,  5.64it/s]\u001b[A\n",
            "Iteration:   4% 318/7834 [00:55<22:18,  5.62it/s]\u001b[A\n",
            "Iteration:   4% 319/7834 [00:55<22:23,  5.59it/s]\u001b[A\n",
            "Iteration:   4% 320/7834 [00:56<22:12,  5.64it/s]\u001b[A\n",
            "Iteration:   4% 321/7834 [00:56<22:17,  5.62it/s]\u001b[A\n",
            "Iteration:   4% 322/7834 [00:56<22:13,  5.63it/s]\u001b[A\n",
            "Iteration:   4% 323/7834 [00:56<22:13,  5.63it/s]\u001b[A\n",
            "Iteration:   4% 324/7834 [00:56<22:08,  5.65it/s]\u001b[A\n",
            "Iteration:   4% 325/7834 [00:57<22:00,  5.69it/s]\u001b[A\n",
            "Iteration:   4% 326/7834 [00:57<21:55,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 327/7834 [00:57<21:52,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 328/7834 [00:57<21:52,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 329/7834 [00:57<21:48,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 330/7834 [00:57<21:48,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 331/7834 [00:58<21:45,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 332/7834 [00:58<21:42,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 333/7834 [00:58<21:45,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 334/7834 [00:58<21:43,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 335/7834 [00:58<21:43,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 336/7834 [00:58<21:41,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 337/7834 [00:59<21:39,  5.77it/s]\u001b[A\n",
            "Iteration:   4% 338/7834 [00:59<21:37,  5.78it/s]\u001b[A\n",
            "Iteration:   4% 339/7834 [00:59<21:49,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 340/7834 [00:59<21:46,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 341/7834 [00:59<21:59,  5.68it/s]\u001b[A\n",
            "Iteration:   4% 342/7834 [00:59<21:56,  5.69it/s]\u001b[A\n",
            "Iteration:   4% 343/7834 [01:00<21:50,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 344/7834 [01:00<21:57,  5.69it/s]\u001b[A\n",
            "Iteration:   4% 345/7834 [01:00<21:50,  5.71it/s]\u001b[A\n",
            "Iteration:   4% 346/7834 [01:00<21:45,  5.74it/s]\u001b[A\n",
            "Iteration:   4% 347/7834 [01:00<21:48,  5.72it/s]\u001b[A\n",
            "Iteration:   4% 348/7834 [01:01<21:47,  5.73it/s]\u001b[A\n",
            "Iteration:   4% 349/7834 [01:01<21:42,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 350/7834 [01:01<21:39,  5.76it/s]\u001b[A\n",
            "Iteration:   4% 351/7834 [01:01<21:40,  5.75it/s]\u001b[A\n",
            "Iteration:   4% 352/7834 [01:01<21:38,  5.76it/s]\u001b[A\n",
            "Iteration:   5% 353/7834 [01:01<21:37,  5.77it/s]\u001b[A\n",
            "Iteration:   5% 354/7834 [01:02<21:46,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 355/7834 [01:02<21:45,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 356/7834 [01:02<21:41,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 357/7834 [01:02<21:39,  5.76it/s]\u001b[A\n",
            "Iteration:   5% 358/7834 [01:02<21:36,  5.77it/s]\u001b[A\n",
            "Iteration:   5% 359/7834 [01:02<21:49,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 360/7834 [01:03<21:46,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 361/7834 [01:03<21:43,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 362/7834 [01:03<21:54,  5.69it/s]\u001b[A\n",
            "Iteration:   5% 363/7834 [01:03<22:02,  5.65it/s]\u001b[A\n",
            "Iteration:   5% 364/7834 [01:03<22:14,  5.60it/s]\u001b[A\n",
            "Iteration:   5% 365/7834 [01:04<22:12,  5.61it/s]\u001b[A\n",
            "Iteration:   5% 366/7834 [01:04<22:02,  5.65it/s]\u001b[A\n",
            "Iteration:   5% 367/7834 [01:04<22:06,  5.63it/s]\u001b[A\n",
            "Iteration:   5% 368/7834 [01:04<21:57,  5.67it/s]\u001b[A\n",
            "Iteration:   5% 369/7834 [01:04<21:51,  5.69it/s]\u001b[A\n",
            "Iteration:   5% 370/7834 [01:04<21:44,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 371/7834 [01:05<21:44,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 372/7834 [01:05<21:40,  5.74it/s]\u001b[A\n",
            "Iteration:   5% 373/7834 [01:05<21:41,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 374/7834 [01:05<21:48,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 375/7834 [01:05<21:59,  5.65it/s]\u001b[A\n",
            "Iteration:   5% 376/7834 [01:05<21:52,  5.68it/s]\u001b[A\n",
            "Iteration:   5% 377/7834 [01:06<21:45,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 378/7834 [01:06<21:41,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 379/7834 [01:06<22:02,  5.64it/s]\u001b[A\n",
            "Iteration:   5% 380/7834 [01:06<21:59,  5.65it/s]\u001b[A\n",
            "Iteration:   5% 381/7834 [01:06<21:51,  5.68it/s]\u001b[A\n",
            "Iteration:   5% 382/7834 [01:06<21:56,  5.66it/s]\u001b[A\n",
            "Iteration:   5% 383/7834 [01:07<21:48,  5.69it/s]\u001b[A\n",
            "Iteration:   5% 384/7834 [01:07<21:43,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 385/7834 [01:07<21:45,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 386/7834 [01:07<21:43,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 387/7834 [01:07<21:53,  5.67it/s]\u001b[A\n",
            "Iteration:   5% 388/7834 [01:08<21:46,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 389/7834 [01:08<21:41,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 390/7834 [01:08<21:37,  5.74it/s]\u001b[A\n",
            "Iteration:   5% 391/7834 [01:08<21:33,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 392/7834 [01:08<21:30,  5.77it/s]\u001b[A\n",
            "Iteration:   5% 393/7834 [01:08<21:35,  5.74it/s]\u001b[A\n",
            "Iteration:   5% 394/7834 [01:09<21:44,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 395/7834 [01:09<21:40,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 396/7834 [01:09<21:36,  5.74it/s]\u001b[A\n",
            "Iteration:   5% 397/7834 [01:09<21:32,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 398/7834 [01:09<21:52,  5.66it/s]\u001b[A\n",
            "Iteration:   5% 399/7834 [01:09<21:46,  5.69it/s]\u001b[A\n",
            "Iteration:   5% 400/7834 [01:10<21:41,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 401/7834 [01:10<21:40,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 402/7834 [01:10<21:44,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 403/7834 [01:10<21:37,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 404/7834 [01:10<21:38,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 405/7834 [01:11<21:32,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 406/7834 [01:11<21:41,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 407/7834 [01:11<21:41,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 408/7834 [01:11<21:36,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 409/7834 [01:11<21:32,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 410/7834 [01:11<21:30,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 411/7834 [01:12<21:27,  5.77it/s]\u001b[A\n",
            "Iteration:   5% 412/7834 [01:12<21:25,  5.77it/s]\u001b[A\n",
            "Iteration:   5% 413/7834 [01:12<21:23,  5.78it/s]\u001b[A\n",
            "Iteration:   5% 414/7834 [01:12<21:23,  5.78it/s]\u001b[A\n",
            "Iteration:   5% 415/7834 [01:12<21:27,  5.76it/s]\u001b[A\n",
            "Iteration:   5% 416/7834 [01:12<21:30,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 417/7834 [01:13<21:37,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 418/7834 [01:13<21:36,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 419/7834 [01:13<21:36,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 420/7834 [01:13<21:36,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 421/7834 [01:13<21:32,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 422/7834 [01:13<21:29,  5.75it/s]\u001b[A\n",
            "Iteration:   5% 423/7834 [01:14<21:26,  5.76it/s]\u001b[A\n",
            "Iteration:   5% 424/7834 [01:14<21:36,  5.71it/s]\u001b[A\n",
            "Iteration:   5% 425/7834 [01:14<21:32,  5.73it/s]\u001b[A\n",
            "Iteration:   5% 426/7834 [01:14<21:40,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 427/7834 [01:14<21:39,  5.70it/s]\u001b[A\n",
            "Iteration:   5% 428/7834 [01:15<21:34,  5.72it/s]\u001b[A\n",
            "Iteration:   5% 429/7834 [01:15<21:29,  5.74it/s]\u001b[A\n",
            "Iteration:   5% 430/7834 [01:15<21:25,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 431/7834 [01:15<21:43,  5.68it/s]\u001b[A\n",
            "Iteration:   6% 432/7834 [01:15<21:38,  5.70it/s]\u001b[A\n",
            "Iteration:   6% 433/7834 [01:15<21:31,  5.73it/s]\u001b[A\n",
            "Iteration:   6% 434/7834 [01:16<21:28,  5.75it/s]\u001b[A\n",
            "Iteration:   6% 435/7834 [01:16<21:26,  5.75it/s]\u001b[A\n",
            "Iteration:   6% 436/7834 [01:16<21:23,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 437/7834 [01:16<21:57,  5.61it/s]\u001b[A\n",
            "Iteration:   6% 438/7834 [01:16<21:46,  5.66it/s]\u001b[A\n",
            "Iteration:   6% 439/7834 [01:16<21:38,  5.69it/s]\u001b[A\n",
            "Iteration:   6% 440/7834 [01:17<21:46,  5.66it/s]\u001b[A\n",
            "Iteration:   6% 441/7834 [01:17<21:41,  5.68it/s]\u001b[A\n",
            "Iteration:   6% 442/7834 [01:17<21:37,  5.70it/s]\u001b[A\n",
            "Iteration:   6% 443/7834 [01:17<21:32,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 444/7834 [01:17<21:30,  5.73it/s]\u001b[A\n",
            "Iteration:   6% 445/7834 [01:18<21:40,  5.68it/s]\u001b[A\n",
            "Iteration:   6% 446/7834 [01:18<21:34,  5.71it/s]\u001b[A\n",
            "Iteration:   6% 447/7834 [01:18<21:28,  5.73it/s]\u001b[A\n",
            "Iteration:   6% 448/7834 [01:18<21:24,  5.75it/s]\u001b[A\n",
            "Iteration:   6% 449/7834 [01:18<21:25,  5.74it/s]\u001b[A\n",
            "Iteration:   6% 450/7834 [01:18<21:21,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 451/7834 [01:19<21:19,  5.77it/s]\u001b[A\n",
            "Iteration:   6% 452/7834 [01:19<21:16,  5.78it/s]\u001b[A\n",
            "Iteration:   6% 453/7834 [01:19<21:16,  5.78it/s]\u001b[A\n",
            "Iteration:   6% 454/7834 [01:19<21:14,  5.79it/s]\u001b[A\n",
            "Iteration:   6% 455/7834 [01:19<21:14,  5.79it/s]\u001b[A\n",
            "Iteration:   6% 456/7834 [01:19<21:14,  5.79it/s]\u001b[A\n",
            "Iteration:   6% 457/7834 [01:20<21:14,  5.79it/s]\u001b[A\n",
            "Iteration:   6% 458/7834 [01:20<21:14,  5.79it/s]\u001b[A\n",
            "Iteration:   6% 459/7834 [01:20<21:15,  5.78it/s]\u001b[A\n",
            "Iteration:   6% 460/7834 [01:20<21:20,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 461/7834 [01:20<21:19,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 462/7834 [01:20<21:20,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 463/7834 [01:21<21:19,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 464/7834 [01:21<21:18,  5.77it/s]\u001b[A\n",
            "Iteration:   6% 465/7834 [01:21<21:17,  5.77it/s]\u001b[A\n",
            "Iteration:   6% 466/7834 [01:21<21:27,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 467/7834 [01:21<21:25,  5.73it/s]\u001b[A\n",
            "Iteration:   6% 468/7834 [01:21<21:28,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 469/7834 [01:22<21:26,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 470/7834 [01:22<21:26,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 471/7834 [01:22<21:28,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 472/7834 [01:22<21:40,  5.66it/s]\u001b[A\n",
            "Iteration:   6% 473/7834 [01:22<21:43,  5.65it/s]\u001b[A\n",
            "Iteration:   6% 474/7834 [01:23<21:33,  5.69it/s]\u001b[A\n",
            "Iteration:   6% 475/7834 [01:23<21:26,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 476/7834 [01:23<21:34,  5.69it/s]\u001b[A\n",
            "Iteration:   6% 477/7834 [01:23<21:27,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 478/7834 [01:23<21:28,  5.71it/s]\u001b[A\n",
            "Iteration:   6% 479/7834 [01:23<21:24,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 480/7834 [01:24<21:20,  5.74it/s]\u001b[A\n",
            "Iteration:   6% 481/7834 [01:24<21:17,  5.76it/s]\u001b[A\n",
            "Iteration:   6% 482/7834 [01:24<21:14,  5.77it/s]\u001b[A\n",
            "Iteration:   6% 483/7834 [01:24<21:29,  5.70it/s]\u001b[A\n",
            "Iteration:   6% 484/7834 [01:24<21:27,  5.71it/s]\u001b[A\n",
            "Iteration:   6% 485/7834 [01:24<21:26,  5.71it/s]\u001b[A\n",
            "Iteration:   6% 486/7834 [01:25<21:42,  5.64it/s]\u001b[A\n",
            "Iteration:   6% 487/7834 [01:25<21:34,  5.68it/s]\u001b[A\n",
            "Iteration:   6% 488/7834 [01:25<21:37,  5.66it/s]\u001b[A\n",
            "Iteration:   6% 489/7834 [01:25<21:31,  5.69it/s]\u001b[A\n",
            "Iteration:   6% 490/7834 [01:25<21:40,  5.65it/s]\u001b[A\n",
            "Iteration:   6% 491/7834 [01:26<21:34,  5.67it/s]\u001b[A\n",
            "Iteration:   6% 492/7834 [01:26<21:37,  5.66it/s]\u001b[A\n",
            "Iteration:   6% 493/7834 [01:26<21:29,  5.69it/s]\u001b[A\n",
            "Iteration:   6% 494/7834 [01:26<21:24,  5.72it/s]\u001b[A\n",
            "Iteration:   6% 495/7834 [01:26<21:19,  5.74it/s]\u001b[A\n",
            "Iteration:   6% 496/7834 [01:26<21:20,  5.73it/s]\u001b[A\n",
            "Iteration:   6% 497/7834 [01:27<21:18,  5.74it/s]\u001b[A\n",
            "Iteration:   6% 498/7834 [01:27<21:33,  5.67it/s]\u001b[A\n",
            "Iteration:   6% 499/7834 [01:27<21:26,  5.70it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/26/2020 21:34:05 - INFO - transformers.configuration_utils -   Configuration saved in distilBert6/checkpoint-500/config.json\n",
            "02/26/2020 21:34:05 - INFO - transformers.modeling_utils -   Model weights saved in distilBert6/checkpoint-500/pytorch_model.bin\n",
            "02/26/2020 21:34:05 - INFO - __main__ -   Saving model checkpoint to distilBert6/checkpoint-500\n",
            "02/26/2020 21:34:07 - INFO - __main__ -   Saving optimizer and scheduler states to distilBert6/checkpoint-500\n",
            "\n",
            "Iteration:   6% 500/7834 [01:30<1:52:40,  1.08it/s]\u001b[A\n",
            "Iteration:   6% 501/7834 [01:30<1:25:24,  1.43it/s]\u001b[A\n",
            "Iteration:   6% 502/7834 [01:30<1:06:24,  1.84it/s]\u001b[A\n",
            "Iteration:   6% 503/7834 [01:30<52:57,  2.31it/s]  \u001b[A\n",
            "Iteration:   6% 504/7834 [01:30<43:30,  2.81it/s]\u001b[A\n",
            "Iteration:   6% 505/7834 [01:30<36:53,  3.31it/s]\u001b[A\n",
            "Iteration:   6% 506/7834 [01:31<32:08,  3.80it/s]\u001b[A\n",
            "Iteration:   6% 507/7834 [01:31<28:51,  4.23it/s]\u001b[A\n",
            "Iteration:   6% 508/7834 [01:31<26:45,  4.56it/s]\u001b[A\n",
            "Iteration:   6% 509/7834 [01:31<25:02,  4.87it/s]\u001b[A\n",
            "Iteration:   7% 510/7834 [01:31<23:57,  5.10it/s]\u001b[A\n",
            "Iteration:   7% 511/7834 [01:32<23:13,  5.26it/s]\u001b[A\n",
            "Iteration:   7% 512/7834 [01:32<22:34,  5.40it/s]\u001b[A\n",
            "Iteration:   7% 513/7834 [01:32<22:14,  5.49it/s]\u001b[A\n",
            "Iteration:   7% 514/7834 [01:32<22:00,  5.55it/s]\u001b[A\n",
            "Iteration:   7% 515/7834 [01:32<21:52,  5.58it/s]\u001b[A\n",
            "Iteration:   7% 516/7834 [01:32<21:40,  5.63it/s]\u001b[A\n",
            "Iteration:   7% 517/7834 [01:33<21:31,  5.66it/s]\u001b[A\n",
            "Iteration:   7% 518/7834 [01:33<21:24,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 519/7834 [01:33<21:23,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 520/7834 [01:33<21:30,  5.67it/s]\u001b[A\n",
            "Iteration:   7% 521/7834 [01:33<21:26,  5.69it/s]\u001b[A\n",
            "Iteration:   7% 522/7834 [01:33<21:21,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 523/7834 [01:34<21:17,  5.72it/s]\u001b[A\n",
            "Iteration:   7% 524/7834 [01:34<21:19,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 525/7834 [01:34<21:23,  5.69it/s]\u001b[A\n",
            "Iteration:   7% 526/7834 [01:34<21:34,  5.65it/s]\u001b[A\n",
            "Iteration:   7% 527/7834 [01:34<21:41,  5.62it/s]\u001b[A\n",
            "Iteration:   7% 528/7834 [01:35<21:44,  5.60it/s]\u001b[A\n",
            "Iteration:   7% 529/7834 [01:35<21:55,  5.55it/s]\u001b[A\n",
            "Iteration:   7% 530/7834 [01:35<21:59,  5.53it/s]\u001b[A\n",
            "Iteration:   7% 531/7834 [01:35<21:49,  5.58it/s]\u001b[A\n",
            "Iteration:   7% 532/7834 [01:35<21:41,  5.61it/s]\u001b[A\n",
            "Iteration:   7% 533/7834 [01:35<21:37,  5.63it/s]\u001b[A\n",
            "Iteration:   7% 534/7834 [01:36<21:25,  5.68it/s]\u001b[A\n",
            "Iteration:   7% 535/7834 [01:36<21:40,  5.61it/s]\u001b[A\n",
            "Iteration:   7% 536/7834 [01:36<21:43,  5.60it/s]\u001b[A\n",
            "Iteration:   7% 537/7834 [01:36<21:44,  5.60it/s]\u001b[A\n",
            "Iteration:   7% 538/7834 [01:36<21:46,  5.58it/s]\u001b[A\n",
            "Iteration:   7% 539/7834 [01:36<21:48,  5.57it/s]\u001b[A\n",
            "Iteration:   7% 540/7834 [01:37<21:52,  5.56it/s]\u001b[A\n",
            "Iteration:   7% 541/7834 [01:37<21:55,  5.54it/s]\u001b[A\n",
            "Iteration:   7% 542/7834 [01:37<22:03,  5.51it/s]\u001b[A\n",
            "Iteration:   7% 543/7834 [01:37<21:47,  5.58it/s]\u001b[A\n",
            "Iteration:   7% 544/7834 [01:37<21:37,  5.62it/s]\u001b[A\n",
            "Iteration:   7% 545/7834 [01:38<21:26,  5.67it/s]\u001b[A\n",
            "Iteration:   7% 546/7834 [01:38<21:18,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 547/7834 [01:38<21:13,  5.72it/s]\u001b[A\n",
            "Iteration:   7% 548/7834 [01:38<21:25,  5.67it/s]\u001b[A\n",
            "Iteration:   7% 549/7834 [01:38<21:17,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 550/7834 [01:38<21:12,  5.72it/s]\u001b[A\n",
            "Iteration:   7% 551/7834 [01:39<21:07,  5.74it/s]\u001b[A\n",
            "Iteration:   7% 552/7834 [01:39<21:07,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 553/7834 [01:39<21:06,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 554/7834 [01:39<21:09,  5.73it/s]\u001b[A\n",
            "Iteration:   7% 555/7834 [01:39<21:08,  5.74it/s]\u001b[A\n",
            "Iteration:   7% 556/7834 [01:39<21:05,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 557/7834 [01:40<21:02,  5.76it/s]\u001b[A\n",
            "Iteration:   7% 558/7834 [01:40<21:00,  5.77it/s]\u001b[A\n",
            "Iteration:   7% 559/7834 [01:40<21:00,  5.77it/s]\u001b[A\n",
            "Iteration:   7% 560/7834 [01:40<21:01,  5.76it/s]\u001b[A\n",
            "Iteration:   7% 561/7834 [01:40<21:04,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 562/7834 [01:41<21:06,  5.74it/s]\u001b[A\n",
            "Iteration:   7% 563/7834 [01:41<21:03,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 564/7834 [01:41<21:18,  5.68it/s]\u001b[A\n",
            "Iteration:   7% 565/7834 [01:41<21:14,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 566/7834 [01:41<21:11,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 567/7834 [01:41<21:24,  5.66it/s]\u001b[A\n",
            "Iteration:   7% 568/7834 [01:42<21:22,  5.67it/s]\u001b[A\n",
            "Iteration:   7% 569/7834 [01:42<21:15,  5.70it/s]\u001b[A\n",
            "Iteration:   7% 570/7834 [01:42<21:20,  5.67it/s]\u001b[A\n",
            "Iteration:   7% 571/7834 [01:42<21:12,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 572/7834 [01:42<21:08,  5.73it/s]\u001b[A\n",
            "Iteration:   7% 573/7834 [01:42<21:12,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 574/7834 [01:43<21:17,  5.68it/s]\u001b[A\n",
            "Iteration:   7% 575/7834 [01:43<21:12,  5.71it/s]\u001b[A\n",
            "Iteration:   7% 576/7834 [01:43<21:07,  5.73it/s]\u001b[A\n",
            "Iteration:   7% 577/7834 [01:43<21:04,  5.74it/s]\u001b[A\n",
            "Iteration:   7% 578/7834 [01:43<21:02,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 579/7834 [01:43<21:01,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 580/7834 [01:44<21:05,  5.73it/s]\u001b[A\n",
            "Iteration:   7% 581/7834 [01:44<21:05,  5.73it/s]\u001b[A\n",
            "Iteration:   7% 582/7834 [01:44<21:00,  5.75it/s]\u001b[A\n",
            "Iteration:   7% 583/7834 [01:44<20:58,  5.76it/s]\u001b[A\n",
            "Iteration:   7% 584/7834 [01:44<20:56,  5.77it/s]\u001b[A\n",
            "Iteration:   7% 585/7834 [01:45<20:57,  5.77it/s]\u001b[A\n",
            "Iteration:   7% 586/7834 [01:45<20:57,  5.76it/s]\u001b[A\n",
            "Iteration:   7% 587/7834 [01:45<21:06,  5.72it/s]\u001b[A\n",
            "Iteration:   8% 588/7834 [01:45<21:17,  5.67it/s]\u001b[A\n",
            "Iteration:   8% 589/7834 [01:45<21:11,  5.70it/s]\u001b[A\n",
            "Iteration:   8% 590/7834 [01:45<21:05,  5.72it/s]\u001b[A\n",
            "Iteration:   8% 591/7834 [01:46<21:03,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 592/7834 [01:46<21:01,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 593/7834 [01:46<20:58,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 594/7834 [01:46<20:57,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 595/7834 [01:46<20:58,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 596/7834 [01:46<20:55,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 597/7834 [01:47<20:55,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 598/7834 [01:47<20:52,  5.78it/s]\u001b[A\n",
            "Iteration:   8% 599/7834 [01:47<21:07,  5.71it/s]\u001b[A\n",
            "Iteration:   8% 600/7834 [01:47<21:03,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 601/7834 [01:47<20:59,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 602/7834 [01:48<20:56,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 603/7834 [01:48<20:54,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 604/7834 [01:48<20:54,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 605/7834 [01:48<20:52,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 606/7834 [01:48<20:57,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 607/7834 [01:48<20:55,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 608/7834 [01:49<20:53,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 609/7834 [01:49<20:54,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 610/7834 [01:49<20:54,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 611/7834 [01:49<21:01,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 612/7834 [01:49<21:08,  5.69it/s]\u001b[A\n",
            "Iteration:   8% 613/7834 [01:49<21:01,  5.72it/s]\u001b[A\n",
            "Iteration:   8% 614/7834 [01:50<20:56,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 615/7834 [01:50<20:55,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 616/7834 [01:50<20:52,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 617/7834 [01:50<21:05,  5.70it/s]\u001b[A\n",
            "Iteration:   8% 618/7834 [01:50<21:01,  5.72it/s]\u001b[A\n",
            "Iteration:   8% 619/7834 [01:50<20:55,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 620/7834 [01:51<20:57,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 621/7834 [01:51<21:10,  5.68it/s]\u001b[A\n",
            "Iteration:   8% 622/7834 [01:51<21:04,  5.70it/s]\u001b[A\n",
            "Iteration:   8% 623/7834 [01:51<20:58,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 624/7834 [01:51<20:54,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 625/7834 [01:52<20:51,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 626/7834 [01:52<20:53,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 627/7834 [01:52<21:04,  5.70it/s]\u001b[A\n",
            "Iteration:   8% 628/7834 [01:52<21:00,  5.72it/s]\u001b[A\n",
            "Iteration:   8% 629/7834 [01:52<20:57,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 630/7834 [01:52<20:53,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 631/7834 [01:53<20:51,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 632/7834 [01:53<20:49,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 633/7834 [01:53<20:49,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 634/7834 [01:53<20:51,  5.75it/s]\u001b[A\n",
            "Iteration:   8% 635/7834 [01:53<20:50,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 636/7834 [01:53<20:48,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 637/7834 [01:54<20:45,  5.78it/s]\u001b[A\n",
            "Iteration:   8% 638/7834 [01:54<20:44,  5.78it/s]\u001b[A\n",
            "Iteration:   8% 639/7834 [01:54<20:52,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 640/7834 [01:54<21:00,  5.71it/s]\u001b[A\n",
            "Iteration:   8% 641/7834 [01:54<20:56,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 642/7834 [01:54<20:52,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 643/7834 [01:55<20:48,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 644/7834 [01:55<20:45,  5.77it/s]\u001b[A\n",
            "Iteration:   8% 645/7834 [01:55<20:43,  5.78it/s]\u001b[A\n",
            "Iteration:   8% 646/7834 [01:55<21:02,  5.69it/s]\u001b[A\n",
            "Iteration:   8% 647/7834 [01:55<21:12,  5.65it/s]\u001b[A\n",
            "Iteration:   8% 648/7834 [01:56<21:21,  5.61it/s]\u001b[A\n",
            "Iteration:   8% 649/7834 [01:56<21:28,  5.58it/s]\u001b[A\n",
            "Iteration:   8% 650/7834 [01:56<21:27,  5.58it/s]\u001b[A\n",
            "Iteration:   8% 651/7834 [01:56<21:29,  5.57it/s]\u001b[A\n",
            "Iteration:   8% 652/7834 [01:56<21:25,  5.59it/s]\u001b[A\n",
            "Iteration:   8% 653/7834 [01:56<21:13,  5.64it/s]\u001b[A\n",
            "Iteration:   8% 654/7834 [01:57<21:02,  5.69it/s]\u001b[A\n",
            "Iteration:   8% 655/7834 [01:57<20:56,  5.71it/s]\u001b[A\n",
            "Iteration:   8% 656/7834 [01:57<20:52,  5.73it/s]\u001b[A\n",
            "Iteration:   8% 657/7834 [01:57<20:58,  5.70it/s]\u001b[A\n",
            "Iteration:   8% 658/7834 [01:57<21:18,  5.61it/s]\u001b[A\n",
            "Iteration:   8% 659/7834 [01:57<21:13,  5.63it/s]\u001b[A\n",
            "Iteration:   8% 660/7834 [01:58<21:02,  5.68it/s]\u001b[A\n",
            "Iteration:   8% 661/7834 [01:58<20:55,  5.71it/s]\u001b[A\n",
            "Iteration:   8% 662/7834 [01:58<20:49,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 663/7834 [01:58<20:45,  5.76it/s]\u001b[A\n",
            "Iteration:   8% 664/7834 [01:58<20:56,  5.71it/s]\u001b[A\n",
            "Iteration:   8% 665/7834 [01:59<21:03,  5.67it/s]\u001b[A\n",
            "Iteration:   9% 666/7834 [01:59<20:56,  5.70it/s]\u001b[A\n",
            "Iteration:   9% 667/7834 [01:59<21:01,  5.68it/s]\u001b[A\n",
            "Iteration:   9% 668/7834 [01:59<20:56,  5.70it/s]\u001b[A\n",
            "Iteration:   9% 669/7834 [01:59<20:52,  5.72it/s]\u001b[A\n",
            "Iteration:   9% 670/7834 [01:59<20:57,  5.70it/s]\u001b[A\n",
            "Iteration:   9% 671/7834 [02:00<20:55,  5.71it/s]\u001b[A\n",
            "Iteration:   9% 672/7834 [02:00<20:50,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 673/7834 [02:00<20:47,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 674/7834 [02:00<20:44,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 675/7834 [02:00<20:43,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 676/7834 [02:00<20:51,  5.72it/s]\u001b[A\n",
            "Iteration:   9% 677/7834 [02:01<21:11,  5.63it/s]\u001b[A\n",
            "Iteration:   9% 678/7834 [02:01<21:01,  5.67it/s]\u001b[A\n",
            "Iteration:   9% 679/7834 [02:01<20:57,  5.69it/s]\u001b[A\n",
            "Iteration:   9% 680/7834 [02:01<20:52,  5.71it/s]\u001b[A\n",
            "Iteration:   9% 681/7834 [02:01<20:48,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 682/7834 [02:01<20:45,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 683/7834 [02:02<20:45,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 684/7834 [02:02<20:44,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 685/7834 [02:02<20:42,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 686/7834 [02:02<20:42,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 687/7834 [02:02<20:41,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 688/7834 [02:03<20:40,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 689/7834 [02:03<20:38,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 690/7834 [02:03<20:49,  5.72it/s]\u001b[A\n",
            "Iteration:   9% 691/7834 [02:03<20:58,  5.67it/s]\u001b[A\n",
            "Iteration:   9% 692/7834 [02:03<21:12,  5.61it/s]\u001b[A\n",
            "Iteration:   9% 693/7834 [02:03<21:37,  5.50it/s]\u001b[A\n",
            "Iteration:   9% 694/7834 [02:04<21:33,  5.52it/s]\u001b[A\n",
            "Iteration:   9% 695/7834 [02:04<21:26,  5.55it/s]\u001b[A\n",
            "Iteration:   9% 696/7834 [02:04<21:14,  5.60it/s]\u001b[A\n",
            "Iteration:   9% 697/7834 [02:04<21:02,  5.65it/s]\u001b[A\n",
            "Iteration:   9% 698/7834 [02:04<20:59,  5.67it/s]\u001b[A\n",
            "Iteration:   9% 699/7834 [02:04<20:51,  5.70it/s]\u001b[A\n",
            "Iteration:   9% 700/7834 [02:05<20:45,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 701/7834 [02:05<20:42,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 702/7834 [02:05<20:39,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 703/7834 [02:05<20:41,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 704/7834 [02:05<20:40,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 705/7834 [02:06<20:42,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 706/7834 [02:06<20:39,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 707/7834 [02:06<20:37,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 708/7834 [02:06<20:39,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 709/7834 [02:06<20:39,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 710/7834 [02:06<20:36,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 711/7834 [02:07<20:40,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 712/7834 [02:07<20:36,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 713/7834 [02:07<20:34,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 714/7834 [02:07<20:44,  5.72it/s]\u001b[A\n",
            "Iteration:   9% 715/7834 [02:07<20:40,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 716/7834 [02:07<20:36,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 717/7834 [02:08<20:37,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 718/7834 [02:08<20:34,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 719/7834 [02:08<20:33,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 720/7834 [02:08<20:33,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 721/7834 [02:08<20:33,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 722/7834 [02:08<20:38,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 723/7834 [02:09<20:35,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 724/7834 [02:09<20:33,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 725/7834 [02:09<20:30,  5.78it/s]\u001b[A\n",
            "Iteration:   9% 726/7834 [02:09<20:30,  5.78it/s]\u001b[A\n",
            "Iteration:   9% 727/7834 [02:09<20:30,  5.78it/s]\u001b[A\n",
            "Iteration:   9% 728/7834 [02:10<20:36,  5.75it/s]\u001b[A\n",
            "Iteration:   9% 729/7834 [02:10<20:33,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 730/7834 [02:10<20:38,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 731/7834 [02:10<20:36,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 732/7834 [02:10<20:37,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 733/7834 [02:10<20:36,  5.74it/s]\u001b[A\n",
            "Iteration:   9% 734/7834 [02:11<20:33,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 735/7834 [02:11<20:31,  5.76it/s]\u001b[A\n",
            "Iteration:   9% 736/7834 [02:11<20:31,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 737/7834 [02:11<20:29,  5.77it/s]\u001b[A\n",
            "Iteration:   9% 738/7834 [02:11<20:41,  5.72it/s]\u001b[A\n",
            "Iteration:   9% 739/7834 [02:11<20:38,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 740/7834 [02:12<20:48,  5.68it/s]\u001b[A\n",
            "Iteration:   9% 741/7834 [02:12<20:47,  5.69it/s]\u001b[A\n",
            "Iteration:   9% 742/7834 [02:12<20:41,  5.71it/s]\u001b[A\n",
            "Iteration:   9% 743/7834 [02:12<20:36,  5.73it/s]\u001b[A\n",
            "Iteration:   9% 744/7834 [02:12<20:35,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 745/7834 [02:12<20:31,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 746/7834 [02:13<20:35,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 747/7834 [02:13<20:32,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 748/7834 [02:13<20:34,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 749/7834 [02:13<20:39,  5.71it/s]\u001b[A\n",
            "Iteration:  10% 750/7834 [02:13<20:40,  5.71it/s]\u001b[A\n",
            "Iteration:  10% 751/7834 [02:14<20:35,  5.73it/s]\u001b[A\n",
            "Iteration:  10% 752/7834 [02:14<20:43,  5.69it/s]\u001b[A\n",
            "Iteration:  10% 753/7834 [02:14<20:40,  5.71it/s]\u001b[A\n",
            "Iteration:  10% 754/7834 [02:14<20:35,  5.73it/s]\u001b[A\n",
            "Iteration:  10% 755/7834 [02:14<20:38,  5.72it/s]\u001b[A\n",
            "Iteration:  10% 756/7834 [02:14<20:49,  5.67it/s]\u001b[A\n",
            "Iteration:  10% 757/7834 [02:15<20:43,  5.69it/s]\u001b[A\n",
            "Iteration:  10% 758/7834 [02:15<20:36,  5.72it/s]\u001b[A\n",
            "Iteration:  10% 759/7834 [02:15<20:34,  5.73it/s]\u001b[A\n",
            "Iteration:  10% 760/7834 [02:15<20:29,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 761/7834 [02:15<20:28,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 762/7834 [02:15<20:33,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 763/7834 [02:16<20:30,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 764/7834 [02:16<20:28,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 765/7834 [02:16<20:27,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 766/7834 [02:16<20:27,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 767/7834 [02:16<20:24,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 768/7834 [02:17<20:25,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 769/7834 [02:17<20:27,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 770/7834 [02:17<20:25,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 771/7834 [02:17<20:25,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 772/7834 [02:17<20:26,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 773/7834 [02:17<20:25,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 774/7834 [02:18<20:28,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 775/7834 [02:18<20:27,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 776/7834 [02:18<20:24,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 777/7834 [02:18<20:27,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 778/7834 [02:18<20:26,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 779/7834 [02:18<20:23,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 780/7834 [02:19<20:23,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 781/7834 [02:19<20:22,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 782/7834 [02:19<20:21,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 783/7834 [02:19<20:20,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 784/7834 [02:19<20:21,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 785/7834 [02:19<20:19,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 786/7834 [02:20<20:18,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 787/7834 [02:20<20:20,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 788/7834 [02:20<20:31,  5.72it/s]\u001b[A\n",
            "Iteration:  10% 789/7834 [02:20<20:29,  5.73it/s]\u001b[A\n",
            "Iteration:  10% 790/7834 [02:20<20:39,  5.68it/s]\u001b[A\n",
            "Iteration:  10% 791/7834 [02:21<20:45,  5.65it/s]\u001b[A\n",
            "Iteration:  10% 792/7834 [02:21<20:36,  5.69it/s]\u001b[A\n",
            "Iteration:  10% 793/7834 [02:21<20:37,  5.69it/s]\u001b[A\n",
            "Iteration:  10% 794/7834 [02:21<20:30,  5.72it/s]\u001b[A\n",
            "Iteration:  10% 795/7834 [02:21<20:26,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 796/7834 [02:21<20:22,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 797/7834 [02:22<20:22,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 798/7834 [02:22<20:19,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 799/7834 [02:22<20:18,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 800/7834 [02:22<20:17,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 801/7834 [02:22<20:15,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 802/7834 [02:22<20:16,  5.78it/s]\u001b[A\n",
            "Iteration:  10% 803/7834 [02:23<20:24,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 804/7834 [02:23<20:22,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 805/7834 [02:23<20:30,  5.71it/s]\u001b[A\n",
            "Iteration:  10% 806/7834 [02:23<20:25,  5.73it/s]\u001b[A\n",
            "Iteration:  10% 807/7834 [02:23<20:23,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 808/7834 [02:23<20:19,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 809/7834 [02:24<20:23,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 810/7834 [02:24<20:33,  5.69it/s]\u001b[A\n",
            "Iteration:  10% 811/7834 [02:24<20:28,  5.72it/s]\u001b[A\n",
            "Iteration:  10% 812/7834 [02:24<20:24,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 813/7834 [02:24<20:20,  5.75it/s]\u001b[A\n",
            "Iteration:  10% 814/7834 [02:25<20:19,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 815/7834 [02:25<20:16,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 816/7834 [02:25<20:17,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 817/7834 [02:25<20:23,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 818/7834 [02:25<20:21,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 819/7834 [02:25<20:17,  5.76it/s]\u001b[A\n",
            "Iteration:  10% 820/7834 [02:26<20:15,  5.77it/s]\u001b[A\n",
            "Iteration:  10% 821/7834 [02:26<20:22,  5.74it/s]\u001b[A\n",
            "Iteration:  10% 822/7834 [02:26<20:24,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 823/7834 [02:26<20:20,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 824/7834 [02:26<20:16,  5.76it/s]\u001b[A\n",
            "Iteration:  11% 825/7834 [02:26<20:17,  5.76it/s]\u001b[A\n",
            "Iteration:  11% 826/7834 [02:27<20:13,  5.77it/s]\u001b[A\n",
            "Iteration:  11% 827/7834 [02:27<20:13,  5.78it/s]\u001b[A\n",
            "Iteration:  11% 828/7834 [02:27<20:12,  5.78it/s]\u001b[A\n",
            "Iteration:  11% 829/7834 [02:27<20:27,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 830/7834 [02:27<20:22,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 831/7834 [02:27<20:18,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 832/7834 [02:28<20:14,  5.77it/s]\u001b[A\n",
            "Iteration:  11% 833/7834 [02:28<20:27,  5.70it/s]\u001b[A\n",
            "Iteration:  11% 834/7834 [02:28<20:22,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 835/7834 [02:28<20:25,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 836/7834 [02:28<20:21,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 837/7834 [02:29<20:17,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 838/7834 [02:29<20:13,  5.77it/s]\u001b[A\n",
            "Iteration:  11% 839/7834 [02:29<20:11,  5.77it/s]\u001b[A\n",
            "Iteration:  11% 840/7834 [02:29<20:18,  5.74it/s]\u001b[A\n",
            "Iteration:  11% 841/7834 [02:29<20:14,  5.76it/s]\u001b[A\n",
            "Iteration:  11% 842/7834 [02:29<20:25,  5.70it/s]\u001b[A\n",
            "Iteration:  11% 843/7834 [02:30<20:23,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 844/7834 [02:30<20:17,  5.74it/s]\u001b[A\n",
            "Iteration:  11% 845/7834 [02:30<20:27,  5.69it/s]\u001b[A\n",
            "Iteration:  11% 846/7834 [02:30<20:38,  5.64it/s]\u001b[A\n",
            "Iteration:  11% 847/7834 [02:30<20:31,  5.67it/s]\u001b[A\n",
            "Iteration:  11% 848/7834 [02:30<20:35,  5.66it/s]\u001b[A\n",
            "Iteration:  11% 849/7834 [02:31<20:28,  5.69it/s]\u001b[A\n",
            "Iteration:  11% 850/7834 [02:31<20:26,  5.69it/s]\u001b[A\n",
            "Iteration:  11% 851/7834 [02:31<20:24,  5.70it/s]\u001b[A\n",
            "Iteration:  11% 852/7834 [02:31<20:21,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 853/7834 [02:31<20:33,  5.66it/s]\u001b[A\n",
            "Iteration:  11% 854/7834 [02:31<20:28,  5.68it/s]\u001b[A\n",
            "Iteration:  11% 855/7834 [02:32<20:36,  5.65it/s]\u001b[A\n",
            "Iteration:  11% 856/7834 [02:32<20:41,  5.62it/s]\u001b[A\n",
            "Iteration:  11% 857/7834 [02:32<20:33,  5.66it/s]\u001b[A\n",
            "Iteration:  11% 858/7834 [02:32<20:32,  5.66it/s]\u001b[A\n",
            "Iteration:  11% 859/7834 [02:32<20:26,  5.69it/s]\u001b[A\n",
            "Iteration:  11% 860/7834 [02:33<20:22,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 861/7834 [02:33<20:19,  5.72it/s]\u001b[A\n",
            "Iteration:  11% 862/7834 [02:33<20:16,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 863/7834 [02:33<20:17,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 864/7834 [02:33<20:15,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 865/7834 [02:33<20:15,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 866/7834 [02:34<20:11,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 867/7834 [02:34<20:21,  5.70it/s]\u001b[A\n",
            "Iteration:  11% 868/7834 [02:34<20:38,  5.63it/s]\u001b[A\n",
            "Iteration:  11% 869/7834 [02:34<20:43,  5.60it/s]\u001b[A\n",
            "Iteration:  11% 870/7834 [02:34<20:46,  5.59it/s]\u001b[A\n",
            "Iteration:  11% 871/7834 [02:35<20:52,  5.56it/s]\u001b[A\n",
            "Iteration:  11% 872/7834 [02:35<20:50,  5.57it/s]\u001b[A\n",
            "Iteration:  11% 873/7834 [02:35<20:51,  5.56it/s]\u001b[A\n",
            "Iteration:  11% 874/7834 [02:35<20:45,  5.59it/s]\u001b[A\n",
            "Iteration:  11% 875/7834 [02:35<20:45,  5.59it/s]\u001b[A\n",
            "Iteration:  11% 876/7834 [02:35<20:32,  5.65it/s]\u001b[A\n",
            "Iteration:  11% 877/7834 [02:36<20:26,  5.67it/s]\u001b[A\n",
            "Iteration:  11% 878/7834 [02:36<20:35,  5.63it/s]\u001b[A\n",
            "Iteration:  11% 879/7834 [02:36<20:28,  5.66it/s]\u001b[A\n",
            "Iteration:  11% 880/7834 [02:36<20:37,  5.62it/s]\u001b[A\n",
            "Iteration:  11% 881/7834 [02:36<20:41,  5.60it/s]\u001b[A\n",
            "Iteration:  11% 882/7834 [02:36<20:45,  5.58it/s]\u001b[A\n",
            "Iteration:  11% 883/7834 [02:37<20:48,  5.57it/s]\u001b[A\n",
            "Iteration:  11% 884/7834 [02:37<20:49,  5.56it/s]\u001b[A\n",
            "Iteration:  11% 885/7834 [02:37<20:51,  5.55it/s]\u001b[A\n",
            "Iteration:  11% 886/7834 [02:37<20:39,  5.61it/s]\u001b[A\n",
            "Iteration:  11% 887/7834 [02:37<20:25,  5.67it/s]\u001b[A\n",
            "Iteration:  11% 888/7834 [02:38<20:16,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 889/7834 [02:38<20:16,  5.71it/s]\u001b[A\n",
            "Iteration:  11% 890/7834 [02:38<20:12,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 891/7834 [02:38<20:12,  5.73it/s]\u001b[A\n",
            "Iteration:  11% 892/7834 [02:38<20:22,  5.68it/s]\u001b[A\n",
            "Iteration:  11% 893/7834 [02:38<20:18,  5.70it/s]\u001b[A\n",
            "Iteration:  11% 894/7834 [02:39<20:12,  5.72it/s]\u001b[A\n",
            "Iteration:  11% 895/7834 [02:39<20:13,  5.72it/s]\u001b[A\n",
            "Iteration:  11% 896/7834 [02:39<20:09,  5.74it/s]\u001b[A\n",
            "Iteration:  11% 897/7834 [02:39<20:07,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 898/7834 [02:39<20:09,  5.74it/s]\u001b[A\n",
            "Iteration:  11% 899/7834 [02:39<20:05,  5.75it/s]\u001b[A\n",
            "Iteration:  11% 900/7834 [02:40<20:02,  5.77it/s]\u001b[A\n",
            "Iteration:  12% 901/7834 [02:40<20:00,  5.78it/s]\u001b[A\n",
            "Iteration:  12% 902/7834 [02:40<20:00,  5.77it/s]\u001b[A\n",
            "Iteration:  12% 903/7834 [02:40<20:02,  5.76it/s]\u001b[A\n",
            "Iteration:  12% 904/7834 [02:40<20:00,  5.77it/s]\u001b[A\n",
            "Iteration:  12% 905/7834 [02:40<20:08,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 906/7834 [02:41<20:05,  5.75it/s]\u001b[A\n",
            "Iteration:  12% 907/7834 [02:41<20:08,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 908/7834 [02:41<20:06,  5.74it/s]\u001b[A\n",
            "Iteration:  12% 909/7834 [02:41<20:03,  5.76it/s]\u001b[A\n",
            "Iteration:  12% 910/7834 [02:41<20:13,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 911/7834 [02:42<20:07,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 912/7834 [02:42<20:21,  5.67it/s]\u001b[A\n",
            "Iteration:  12% 913/7834 [02:42<20:19,  5.67it/s]\u001b[A\n",
            "Iteration:  12% 914/7834 [02:42<20:15,  5.69it/s]\u001b[A\n",
            "Iteration:  12% 915/7834 [02:42<20:08,  5.72it/s]\u001b[A\n",
            "Iteration:  12% 916/7834 [02:42<20:11,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 917/7834 [02:43<20:10,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 918/7834 [02:43<20:05,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 919/7834 [02:43<20:10,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 920/7834 [02:43<20:06,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 921/7834 [02:43<20:05,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 922/7834 [02:43<20:02,  5.75it/s]\u001b[A\n",
            "Iteration:  12% 923/7834 [02:44<20:00,  5.76it/s]\u001b[A\n",
            "Iteration:  12% 924/7834 [02:44<19:57,  5.77it/s]\u001b[A\n",
            "Iteration:  12% 925/7834 [02:44<19:55,  5.78it/s]\u001b[A\n",
            "Iteration:  12% 926/7834 [02:44<19:55,  5.78it/s]\u001b[A\n",
            "Iteration:  12% 927/7834 [02:44<19:54,  5.78it/s]\u001b[A\n",
            "Iteration:  12% 928/7834 [02:44<19:52,  5.79it/s]\u001b[A\n",
            "Iteration:  12% 929/7834 [02:45<19:52,  5.79it/s]\u001b[A\n",
            "Iteration:  12% 930/7834 [02:45<20:01,  5.75it/s]\u001b[A\n",
            "Iteration:  12% 931/7834 [02:45<20:13,  5.69it/s]\u001b[A\n",
            "Iteration:  12% 932/7834 [02:45<20:09,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 933/7834 [02:45<20:05,  5.72it/s]\u001b[A\n",
            "Iteration:  12% 934/7834 [02:46<20:02,  5.74it/s]\u001b[A\n",
            "Iteration:  12% 935/7834 [02:46<20:01,  5.74it/s]\u001b[A\n",
            "Iteration:  12% 936/7834 [02:46<20:07,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 937/7834 [02:46<20:08,  5.71it/s]\u001b[A\n",
            "Iteration:  12% 938/7834 [02:46<20:02,  5.73it/s]\u001b[A\n",
            "Iteration:  12% 939/7834 [02:46<20:01,  5.74it/s]\u001b[A\n",
            "Iteration:  12% 940/7834 [02:47<20:16,  5.67it/s]\u001b[A\n",
            "Iteration:  12% 941/7834 [02:47<20:13,  5.68it/s]\u001b[A\n",
            "Iteration:  12% 942/7834 [02:47<20:15,  5.67it/s]\u001b[A\n",
            "Iteration:  12% 943/7834 [02:47<20:17,  5.66it/s]\u001b[A\n",
            "Iteration:  12% 944/7834 [02:47<20:08,  5.70it/s]\u001b[A\n",
            "Iteration:  12% 945/7834 [02:47<20:04,  5.72it/s]\u001b[A\n",
            "Iteration:  12% 946/7834 [02:48<20:04,  5.72it/s]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7zojUjomBZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r distilBert6 drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpmuSzN6Qmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_json(\"drive/My Drive/Hinglish/clean_data/train.json\")\n",
        "test_df = pd.read_json(\"drive/My Drive/Hinglish/clean_data/test.json\")\n",
        "sentences = train_df['clean_text']\n",
        "labels = train_df['sentiment']\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(labels)\n",
        "labels = le.transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7GPInw4Y9zm",
        "colab_type": "code",
        "outputId": "9169b92e-a69a-4bc0-de05-eb30be7031d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwe36B8bY-VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -r output drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_zp7qkIZEyh",
        "colab_type": "code",
        "outputId": "2778e7e8-43c1-4b46-b9bd-9d3c6803e790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Load the DistilBERT tokenizer.\n",
        "print('Loading DistilBERT tokenizer...')\n",
        "tokenizer= DistilBertTokenizer.from_pretrained(\"drive/My Drive/distilBert6\")\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "\n",
        "print (tokenized_texts[0])\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 128\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning DistilBERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
        "\n",
        "# Load DistilBertForSequenceClassification, the pretrained DistilBERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"drive/My Drive/distilBert6\", # Use the 12-layer DistilBERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification. \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading DistilBERT tokenizer...\n",
            "Tokenize the first sentence:\n",
            "['@', 'c', '##yr', '##us', '##gos', '##h', 'I', 'CA', '##N', '’', 'T', 'E', '##VE', '##N', 'B', '##EL', '##IE', '##VE', 'T', '##HA', '##T', 'I', '’', 'V', '##E', 'GO', '##T', 'P', '##IC', '##TU', '##RE', '##S', 'W', '##IT', '##H', 'MI', '##LE', '##Y', 'F', '##UC', '##K', '##ING', 'C', '##Y', '##R', '##US', 'T', '##H', '##IS', 'IS', 'A', 'F', '##UC', '##K', '##ING', 'DR', '##EA', '##M', 'CO', '##ME', 'T', '##R', '##UE', ':', 'loudly', '_', 'crying', '_', 'face', ':', 'I', 'L', '##O', '##VE', 'YOU', 'M', '##OR', '##E', 'T', '##H', '…']\n",
            "Original:   @ cyrusgosh I CAN ’ T EVEN BELIEVE THAT I ’ VE GOT PICTURES WITH MILEY FUCKING CYRUS THIS IS A FUCKING DREAM COME TRUE :loudly_crying_face: I LOVE YOU MORE TH …  \n",
            "Token IDs: [101, 137, 172, 12577, 1361, 12443, 1324, 146, 8784, 2249, 787, 157, 142, 17145, 2249, 139, 21678, 17444, 17145, 157, 11612, 1942, 146, 787, 159, 2036, 27157, 1942, 153, 9741, 27074, 16941, 1708, 160, 12150, 3048, 26574, 17516, 3663, 143, 21986, 2428, 15740, 140, 3663, 2069, 13329, 157, 3048, 6258, 19432, 138, 143, 21986, 2428, 15740, 22219, 12420, 2107, 18732, 14424, 157, 2069, 24846, 131, 9733, 168, 6675, 168, 1339, 131, 146, 149, 2346, 17145, 19141, 150, 9565, 2036, 157, 3048, 795, 102]\n",
            "\n",
            "Padding/truncating all sentences to 128 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkoCV0gPZeGD",
        "colab_type": "code",
        "outputId": "58bc829a-62b9-47c4-ce73-a46c9be1fab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_prf(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return precision_recall_fscore_support(labels_flat, pred_flat, labels =[0,1,2],average='macro')\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 104 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "distilbert.embeddings.word_embeddings.weight            (28996, 768)\n",
            "distilbert.embeddings.position_embeddings.weight          (512, 768)\n",
            "distilbert.embeddings.LayerNorm.weight                        (768,)\n",
            "distilbert.embeddings.LayerNorm.bias                          (768,)\n",
            "distilbert.transformer.layer.0.attention.q_lin.weight     (768, 768)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "distilbert.transformer.layer.0.attention.q_lin.bias           (768,)\n",
            "distilbert.transformer.layer.0.attention.k_lin.weight     (768, 768)\n",
            "distilbert.transformer.layer.0.attention.k_lin.bias           (768,)\n",
            "distilbert.transformer.layer.0.attention.v_lin.weight     (768, 768)\n",
            "distilbert.transformer.layer.0.attention.v_lin.bias           (768,)\n",
            "distilbert.transformer.layer.0.attention.out_lin.weight   (768, 768)\n",
            "distilbert.transformer.layer.0.attention.out_lin.bias         (768,)\n",
            "distilbert.transformer.layer.0.sa_layer_norm.weight           (768,)\n",
            "distilbert.transformer.layer.0.sa_layer_norm.bias             (768,)\n",
            "distilbert.transformer.layer.0.ffn.lin1.weight           (3072, 768)\n",
            "distilbert.transformer.layer.0.ffn.lin1.bias                 (3072,)\n",
            "distilbert.transformer.layer.0.ffn.lin2.weight           (768, 3072)\n",
            "distilbert.transformer.layer.0.ffn.lin2.bias                  (768,)\n",
            "distilbert.transformer.layer.0.output_layer_norm.weight       (768,)\n",
            "distilbert.transformer.layer.0.output_layer_norm.bias         (768,)\n",
            "distilbert.transformer.layer.1.attention.q_lin.weight     (768, 768)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "pre_classifier.weight                                     (768, 768)\n",
            "pre_classifier.bias                                           (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnLhPlF5d_RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evh4C0LAbF0W",
        "colab_type": "code",
        "outputId": "96337225-9d68-4bad-cf3c-bcc302da98e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_p = 0\n",
        "    eval_r = 0\n",
        "    eval_f1 = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,  \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        temp_eval_f1 = flat_prf(logits,label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        eval_p += temp_eval_f1[0]\n",
        "        eval_r += temp_eval_f1[1]\n",
        "        eval_f1 += temp_eval_f1[2]\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(f\"  Precision, Recall F1: {eval_p/nb_eval_steps}, {eval_r/nb_eval_steps}, {eval_f1/nb_eval_steps}\")\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    383.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    383.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    383.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    383.    Elapsed: 0:00:32.\n",
            "  Batch   200  of    383.    Elapsed: 0:00:40.\n",
            "  Batch   240  of    383.    Elapsed: 0:00:48.\n",
            "  Batch   280  of    383.    Elapsed: 0:00:56.\n",
            "  Batch   320  of    383.    Elapsed: 0:01:04.\n",
            "  Batch   360  of    383.    Elapsed: 0:01:12.\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Precision, Recall F1: 0.6265050948190484, 0.6276811442851118, 0.6117684986835866\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    383.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    383.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    383.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    383.    Elapsed: 0:00:32.\n",
            "  Batch   200  of    383.    Elapsed: 0:00:40.\n",
            "  Batch   240  of    383.    Elapsed: 0:00:47.\n",
            "  Batch   280  of    383.    Elapsed: 0:00:55.\n",
            "  Batch   320  of    383.    Elapsed: 0:01:03.\n",
            "  Batch   360  of    383.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Precision, Recall F1: 0.6362283173329685, 0.6315632567513553, 0.6198694437284944\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    383.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    383.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    383.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    383.    Elapsed: 0:00:32.\n",
            "  Batch   200  of    383.    Elapsed: 0:00:40.\n",
            "  Batch   240  of    383.    Elapsed: 0:00:47.\n",
            "  Batch   280  of    383.    Elapsed: 0:00:55.\n",
            "  Batch   320  of    383.    Elapsed: 0:01:03.\n",
            "  Batch   360  of    383.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Precision, Recall F1: 0.6413431592980157, 0.634992860283558, 0.6238375191885951\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    383.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    383.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    383.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    383.    Elapsed: 0:00:32.\n",
            "  Batch   200  of    383.    Elapsed: 0:00:39.\n",
            "  Batch   240  of    383.    Elapsed: 0:00:47.\n",
            "  Batch   280  of    383.    Elapsed: 0:00:55.\n",
            "  Batch   320  of    383.    Elapsed: 0:01:03.\n",
            "  Batch   360  of    383.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Precision, Recall F1: 0.642397513567144, 0.6284228400165474, 0.6200827057119507\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYHoaYojbTDR",
        "colab_type": "code",
        "outputId": "c7a5068e-9e80-4dc7-fcb3-b87022b11a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bQzLed7hFaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = test_df['clean_text']\n",
        "labels = test_df['sentiment']\n",
        "labels = le.transform(labels)\n",
        "# Report the number of sentences.\n",
        "\n",
        "# Create sentence and label lists\n",
        "# sentences = df.sentence.values\n",
        "# labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_uNjBvEh5xt",
        "colab_type": "code",
        "outputId": "1a6643a6-2c11-4522-c563-446722d93110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 3,400 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF2ZHHMah8oC",
        "colab_type": "code",
        "outputId": "b1159841-c2df-4489-f9a0-b51f2c4138cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)\n",
        "\n",
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(flat_predictions, flat_true_labels, average='macro')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n",
            "MCC: 0.457\n",
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6441705246013023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J7h_oNijN-M",
        "colab_type": "code",
        "outputId": "91294af6-0c2a-4fa8-fcc3-c132bbb56186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "final_test_df = pd.read_json(\"drive/My Drive/Hinglish/clean_data/final_test.json\")\n",
        "sentences = final_test_df['clean_text']\n",
        "# Report the number of sentences.\n",
        "\n",
        "# Create sentence and label lists\n",
        "# sentences = df.sentence.values\n",
        "# labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on valid set\n",
        "\n",
        "print('Predicting labels for {:,} valid sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "\n",
        "print('    DONE.')\n",
        "\n",
        "\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,999 valid sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLkx6qkyja-1",
        "colab_type": "code",
        "outputId": "8fb8a252-860b-4965-f488-bddff77da64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(flat_predictions.tolist())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP-eaAgOjyV6",
        "colab_type": "code",
        "outputId": "d297862a-edc7-467e-f14a-16910733256a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "output = le.inverse_transform(flat_predictions.tolist())\n",
        "output_df = pd.DataFrame({\n",
        "    'Uid':list(final_test_df['uid']),\n",
        "    'Sentiment':output,\n",
        "    'clean_text':list(final_test_df['clean_text'])\n",
        "})\n",
        "\n",
        "output_df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Uid</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20803</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@ 454dkhan @ Heisunberg _ Agr kse ko itni impo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20187</td>\n",
              "      <td>neutral</td>\n",
              "      <td>logon ko alloo pyaz tomator me toh allah pak k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20953</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@ LambaAlka Wafadaar bane rahane ka nayab tari...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13777</td>\n",
              "      <td>negative</td>\n",
              "      <td>@ varnishant @ narendramodi Chup bhosdike . He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20990</td>\n",
              "      <td>positive</td>\n",
              "      <td>@ HardeepSPuri à¨¦à¨¾à¨¤à¨¾ à¨¸à:copyright:‹ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>7026</td>\n",
              "      <td>positive</td>\n",
              "      <td>@ narendramodi Pradhanmantri ji aap ko dusri B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>10425</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@ i _ yogesh22 @ atulreellife Haha @ atulreell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>14162</td>\n",
              "      <td>positive</td>\n",
              "      <td>ðŸ:trade_mark:ðŸ:trade_mark:ðŸ:trade_mark: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>15860</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@ Nimra _ Khan241 Fahad Bhaiiiii give us Bila...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>25435</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@ fralaliciouxxe tbh i have bad sides too . w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2999 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Uid Sentiment                                         clean_text\n",
              "0     20803   neutral  @ 454dkhan @ Heisunberg _ Agr kse ko itni impo...\n",
              "1     20187   neutral  logon ko alloo pyaz tomator me toh allah pak k...\n",
              "2     20953   neutral  @ LambaAlka Wafadaar bane rahane ka nayab tari...\n",
              "3     13777  negative  @ varnishant @ narendramodi Chup bhosdike . He...\n",
              "4     20990  positive   @ HardeepSPuri à¨¦à¨¾à¨¤à¨¾ à¨¸à:copyright:‹ ...\n",
              "...     ...       ...                                                ...\n",
              "2994   7026  positive  @ narendramodi Pradhanmantri ji aap ko dusri B...\n",
              "2995  10425   neutral  @ i _ yogesh22 @ atulreellife Haha @ atulreell...\n",
              "2996  14162  positive  ðŸ:trade_mark:ðŸ:trade_mark:ðŸ:trade_mark: ...\n",
              "2997  15860   neutral   @ Nimra _ Khan241 Fahad Bhaiiiii give us Bila...\n",
              "2998  25435   neutral   @ fralaliciouxxe tbh i have bad sides too . w...\n",
              "\n",
              "[2999 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofSMwGVZjsdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('answer.txt', 'w') as f:\n",
        "    for i in range(len(output.tolist())):\n",
        "        f.write(f\"{validdf.loc[i]['uid']},{output[i]}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdmVQ_ZbGq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}