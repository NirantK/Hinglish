{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/radhikasethi2011/Hinglish/blob/data_exploration/HinglishNLP_Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \r\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CleanClassFiles = json files \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive/CleanClassFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \r\n",
    "p = Path('.')\r\n",
    "allFiles = []\r\n",
    "for json_file in p.iterdir():\r\n",
    "  allFiles.append(json_file)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\r\n",
    "result = [] \r\n",
    "for json_file in allFiles:\r\n",
    "  input_file = open(json_file,'r')\r\n",
    "  f = open(json_file)\r\n",
    "  decoded_text = json.load(f)\r\n",
    "  for item in decoded_text:\r\n",
    "    text_data = item.get('clean_text')\r\n",
    "    print(text_data)\r\n",
    "    result.append(text_data)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/json_clean_text.txt', 'w') as outfile:\r\n",
    "  outfile.write(\"Text\\n\")\r\n",
    "  for r in result:\r\n",
    "    outfile.write(\"%s\\n\" % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning json_clean_text.txt data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \r\n",
    "import re\r\n",
    "df = pd.read_csv('/content/gdrive/MyDrive/DataExploration/json_clean_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\r\n",
    "pd.set_option('display.max_columns', None)\r\n",
    "pd.set_option('display.width', None)\r\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, col):\r\n",
    "    \"\"\"Cleaning Twiitter data\r\n",
    "    Arguments:\r\n",
    "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\r\n",
    "        col {[string]} -- column in which text is present\r\n",
    "    Returns:\r\n",
    "        [pandas dataframe] -- Datframe with a \"clean_text\" column\r\n",
    "    \"\"\"\r\n",
    "    df[\"clean_text\"] = df[col]\r\n",
    "    df[\"clean_text\"] = (\r\n",
    "        (df[\"clean_text\"])\r\n",
    "        .apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"Retweet\", text))  # Removes RTS\r\n",
    "        .apply(lambda text: re.sub(r\"@\", \"mention \", text))  # Replaces @ with mention\r\n",
    "        .apply(lambda text: re.sub(r\"#\", \"hashtag \", text))  # Replaces # with hastag\r\n",
    "        .apply(lambda text: re.sub(r\"http\\S+\", \"\", text))  # Removes URL\r\n",
    "        .apply(lambda text: re.sub(r'// .*$', \":\", text)\r\n",
    "        \r\n",
    "    )\r\n",
    "    return df\r\n",
    "\r\n",
    "df1 = clean(df, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving clean json text as jclean_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "np.savetxt(r'/content/gdrive/MyDrive/DataExploration/jclean_text.txt', df1['clean_text'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatinating jclean_text.txt with 3 text files in CleanLMFiles\r\n",
    "1. concat CleanLM Files \r\n",
    "2. Concat giga_CleanLM File with cleaned_json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/CleanLMFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('.')\r\n",
    "allFiles = []\r\n",
    "for text_file in p.iterdir():\r\n",
    "  allFiles.append(text_file)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/giga_text.txt', 'w') as outfile:\r\n",
    "  outfile.write(\"Text\\n\")\r\n",
    "  for f in allFiles:\r\n",
    "    with open(f) as infile:\r\n",
    "      contents = infile.read()\r\n",
    "      outfile.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\r\n",
    "%cd /content/gdrive/MyDrive/DataExploration/\r\n",
    "\r\n",
    "p = Path('.')\r\n",
    "allFiles = []\r\n",
    "for text_file in p.iterdir():\r\n",
    "  if not text_file.is_dir():\r\n",
    "    allFiles.append(text_file)\r\n",
    "\r\n",
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/MegaTextDoc.txt', 'w') as outfile:\r\n",
    "  outfile.write(\"Text\\n\")\r\n",
    "  for f in allFiles:\r\n",
    "    with open(f) as infile:\r\n",
    "      contents = infile.read()\r\n",
    "      outfile.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mega file done. \r\n",
    "Data exploration! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of stop words from [HinglishNLP](https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/DataExploration/\r\n",
    "stopwords = []\r\n",
    "text_file = open('stopwords-hinglish.txt', 'r')\r\n",
    "stopwords = [line.split(\"\\n\")[0] for line in text_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords #stopwords list from hinglish-stopwords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/DataExploration/mega_text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "df = pd.read_csv('MegaTextDoc.txt', sep='delimiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#everything in lowertext \r\n",
    "df['Text'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of chars per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords) #set of the hinglish stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.update(['Retweet','retweet','mention',':','.','...','....', '//','-','/','!','?','hashtag', '_','â€¦','\"\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus \r\n",
    "corpus1 = []\r\n",
    "new= df['Text'].str.split()\r\n",
    "new=new.values.tolist()\r\n",
    "corpus1=[word for i in new for word in i]\r\n",
    "\r\n",
    "#dictionary of stop words\r\n",
    "from collections import defaultdict\r\n",
    "dic=defaultdict(int)\r\n",
    "for word in corpus1:\r\n",
    "    if word in stop:\r\n",
    "        dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new #list of lists of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop))\r\n",
    "print(len(corpus))\r\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the most common stopwords in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \r\n",
    "x,y=zip(*top)\r\n",
    "plt.bar(x,y)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding words other than stopwords that occur frequently in the hinglish data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\r\n",
    "from collections import Counter\r\n",
    "counter=Counter(corpus1)\r\n",
    "most=counter.most_common()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most #most common words from data eliminating the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for most common words excluding stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y= [], []\r\n",
    "\r\n",
    "\r\n",
    "for word,count in most[:100]:\r\n",
    "  if (word not in stop):\r\n",
    "    x.append(word)\r\n",
    "    y.append(count)\r\n",
    "        \r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the top bi-grams in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "def get_top_ngram(corpus1, n=None):\r\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus1)\r\n",
    "    bag_of_words = vec.transform(corpus1)\r\n",
    "    sum_words = bag_of_words.sum(axis=0) \r\n",
    "    words_freq = [(word, sum_words[0, idx]) \r\n",
    "                  for word, idx in vec.vocabulary_.items()]\r\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\r\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the top tri-grams in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bi_grams=get_top_ngram(df['Text'],n=2)\r\n",
    "x,y=map(list,zip(*top_bi_grams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tri_grams=get_top_ngram(df['Text'],n=3)\r\n",
    "x,y=map(list,zip(*top_tri_grams))\r\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modelling \r\n",
    "- tokenize \r\n",
    "- removing stopwords \r\n",
    "- lemmatize \r\n",
    "- bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].fillna('').apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize \r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('wordnet')\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "def preprocess_data(df):\r\n",
    "    corpus=[]\r\n",
    "    stem=PorterStemmer()\r\n",
    "    lem=WordNetLemmatizer()\r\n",
    "    #and re.match('^[a-zA-Z]+', w)\r\n",
    "    for text in df['Text']:\r\n",
    "      words=[w for w in word_tokenize(text) if (w not in stop)]\r\n",
    "      words=[lem.lemmatize(w) for w in words if len(w)>2]\r\n",
    "      corpus.append(words)\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus=preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words - gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\r\n",
    "\r\n",
    "dic=gensim.corpora.Dictionary(corpus)\r\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Model \r\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 1, workers = 4)\r\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyLDAvis\r\n",
    "import pyLDAvis\r\n",
    "from pyLDAvis import gensim\r\n",
    "\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\r\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Analysis - on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vocabulary Size - Number of unique words \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set(x for l in new for x in l)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Top 100 words by frequency - removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords \r\n",
    "counter1=Counter(corpus1)\r\n",
    "most1=counter1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without stopwords \r\n",
    "dic1=defaultdict(int)\r\n",
    "for word in corpus1:\r\n",
    "    if word not in stop:\r\n",
    "        dic1[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 frequent words \r\n",
    "\r\n",
    "c = Counter(dic1)\r\n",
    "top_100 = c.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of top 10 words in the data, excluding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=sorted(dic1.items(), key=lambda x:x[1],reverse=True)[:10] \r\n",
    "x,y=zip(*top)\r\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=sorted(dic1.items(), key=lambda x:x[1],reverse=True)[:500]\n",
    "x,y=zip(*top)\n",
    "#plt.subplots(figsize=(18,10))\n",
    "plt.figure(figsize=(18,100)) \n",
    "plt.barh(x,y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
