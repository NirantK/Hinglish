{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating the Labels as Noisy\n",
    "### Learning from Noisy Labels using CleanLab\n",
    "\n",
    "Link: https://github.com/cgnorthcutt/cleanlab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@ AdilNisarButt pakistan ka ghra tauq he Pakis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>negative</td>\n",
       "      <td>Madarchod mulle ye mathura me Nahi dikha tha j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>positive</td>\n",
       "      <td>@ narendramodi Manya Pradhan Mantri mahoday Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>positive</td>\n",
       "      <td>@ Atheist _ Krishna Jcb full trend me chal rah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>positive</td>\n",
       "      <td>@ AbhisharSharma _ @ RavishKumarBlog Loksabha ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid sentiment                                               text\n",
       "0    3  negative  @ AdilNisarButt pakistan ka ghra tauq he Pakis...\n",
       "1   41  negative  Madarchod mulle ye mathura me Nahi dikha tha j...\n",
       "2   48  positive  @ narendramodi Manya Pradhan Mantri mahoday Sh...\n",
       "3   64  positive  @ Atheist _ Krishna Jcb full trend me chal rah...\n",
       "4   66  positive  @ AbhisharSharma _ @ RavishKumarBlog Loksabha ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('data/raw/train.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(list(train['sentiment']))); num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refering to : [Twitter-Airlines](https://github.com/martinpella/twitter-airlines/blob/master/shallow_learning.ipynb) for cleaning data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade snowballstemmer\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TextCleaner, CleanTwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 77.2 ms, total: 24.4 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%time X_train, X_test, y_train, y_test, tfidf_train, tfidf_test = CleanTwitter(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningWithNoisyLabels : [IrisSimple](https://github.com/cgnorthcutt/cleanlab/blob/master/examples/iris_simple_example.ipynb)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: cleanlab in /Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages (0.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages (from cleanlab) (1.17.4)\r\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.18 in /Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages (from cleanlab) (0.22)\r\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages (from cleanlab) (1.4.0)\r\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages (from scikit-learn>=0.18->cleanlab) (0.14.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meghanabhange/anaconda3/envs/Hinglish/lib/python3.7/site-packages/cleanlab/pruning.py:32: UserWarning: If you want to see estimated completion times\n",
      "    while running methods in cleanlab.pruning, install tqdm\n",
      "    via \"pip install tqdm\".\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "from cleanlab.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.noise_generation import generate_noisy_labels\n",
    "from cleanlab.util import value_counts\n",
    "from cleanlab.latent_algebra import compute_inv_noise_matrix\n",
    "import cleanlab\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "np.random.seed(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if this is how you do it? HOW TO CHANGE THESE VALUES?\n",
    "\n",
    "# Set the sparsity of the noise matrix.\n",
    "FRAC_ZERO_NOISE_RATES = 0.5\n",
    "# A proxy for the fraction of labels that are correct.\n",
    "avg_trace = 0.67 # ~33% wrong labels. Increasing makes the problem easier.\n",
    "\n",
    "p_y = value_counts(y_train) # probability of y estimates\n",
    "noise_matrix = generate_noise_matrix_from_trace(\n",
    "        K = num_classes,\n",
    "        trace = num_classes * avg_trace, \n",
    "        py = p_y,\n",
    "        frac_zero_noise_rates = FRAC_ZERO_NOISE_RATES,\n",
    "    )\n",
    "\n",
    "# Create noisy labels\n",
    "s = generate_noisy_labels(y_train, noise_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57010869, 0.        , 0.        ],\n",
       "       [0.        , 0.91519804, 0.47530674],\n",
       "       [0.42989131, 0.08480196, 0.52469326]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT confident learning, test accuracy: 0.59397\n"
     ]
    }
   ],
   "source": [
    "print('WITHOUT confident learning,', end=\" \")\n",
    "m = LogisticRegression(C=4, dual=False, multi_class=\"auto\", solver=\"lbfgs\", max_iter=1000)\n",
    "_ = m.fit(tfidf_train, y_train)\n",
    "pred = m.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH confident learning (without noise matrix given), test accuracy: 0.60322\n"
     ]
    }
   ],
   "source": [
    "print('WITH confident learning (without noise matrix given),', end=\" \")\n",
    "m_rp = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf = m_rp)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH confident learning (noise matrix given), test accuracy: 0.61089\n"
     ]
    }
   ],
   "source": [
    "print('WITH confident learning (noise matrix given),', end=\" \")\n",
    "m2_rp = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf = m2_rp)\n",
    "_ = rp.fit(tfidf_train, y_train, noise_matrix=noise_matrix)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH confident learning (noise / inverse noise matrix given), test accuracy: 0.6101\n"
     ]
    }
   ],
   "source": [
    "print('WITH confident learning (noise / inverse noise matrix given),', end=\" \")\n",
    "m3_rp = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf = m3_rp)\n",
    "_ = rp.fit(tfidf_train, y_train, noise_matrix = noise_matrix, inverse_noise_matrix=compute_inv_noise_matrix(p_y, noise_matrix))\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH confident learning (using latent noise matrix estimation), test accuracy: 0.60508\n"
     ]
    }
   ],
   "source": [
    "print('WITH confident learning (using latent noise matrix estimation),', end=\" \")\n",
    "m = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf = m)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH confident learning (using calibrated confident joint), test accuracy: 0.60878\n"
     ]
    }
   ],
   "source": [
    "print('WITH confident learning (using calibrated confident joint),', end=\" \")\n",
    "m = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(s != y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade hypopt # this is not a typo, hypopt and hyperopt are different packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hypopt.model_selection import GridSearch\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param settings: {'prune_method': 'prune_by_class', 'converge_latent_estimates': False}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.45995 \n",
      "\n",
      "Param settings: {'prune_method': 'both', 'converge_latent_estimates': False}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.45969 \n",
      "\n",
      "Param settings: {'prune_method': 'both', 'converge_latent_estimates': True}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.45942 \n",
      "\n",
      "Param settings: {'prune_method': 'prune_by_noise_rate', 'converge_latent_estimates': True}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.45731 \n",
      "\n",
      "Param settings: {'prune_method': 'prune_by_class', 'converge_latent_estimates': True}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.45467 \n",
      "\n",
      "Param settings: {'prune_method': 'prune_by_noise_rate', 'converge_latent_estimates': False}\n",
      "Hinglish dataset test accuracy (using confident learning):\t 0.4507 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"prune_method\": [\"prune_by_noise_rate\", \"prune_by_class\", \"both\"],\n",
    "    \"converge_latent_estimates\": [True, False],\n",
    "}\n",
    "# Fit LearningWithNoisyLabels across all parameter settings.\n",
    "params = ParameterGrid(param_grid)\n",
    "scores = []\n",
    "for param in params:\n",
    "    clf = LogisticRegression(solver = 'lbfgs', multi_class = 'auto', max_iter = 1000)\n",
    "    rp = LearningWithNoisyLabels(clf = clf, **param)\n",
    "    _ = rp.fit(tfidf_train, s) # s is the noisy y_train labels\n",
    "    scores.append(accuracy_score(rp.predict(tfidf_test), y_test))\n",
    "\n",
    "# Print results sorted from best to least\n",
    "for i in np.argsort(scores)[::-1]:\n",
    "    print(\"Param settings:\", params[i])\n",
    "    print(\n",
    "        \"Hinglish dataset test accuracy (using confident learning):\\t\", \n",
    "        round(scores[i], 5),\n",
    "        \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 Most Likely Wrong Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/cgnorthcutt/rankpruning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "psx = cleanlab.latent_estimation.estimate_cv_predicted_probabilities(tfidf_train, s, clf=LogisticRegression(max_iter=1000, multi_class='auto', solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.asarray(s)\n",
    "psx = np.asarray(psx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "import cleanlab\n",
    "\n",
    "jc, psx = cleanlab.latent_estimation.estimate_confident_joint_and_cv_pred_proba(tfidf_train, y_train, rp)\n",
    "\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s = s,\n",
    "    psx = psx,\n",
    "    sorted_index_method='normalized_margin',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3647, 1849,  417, ..., 2706, 7614, 1452])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_label_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>husne ylcnk romagnetique eloraxiong i love you</td>\n",
       "      <td>1</td>\n",
       "      <td>3647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anuview badhai badhai badhai ji haan you are u...</td>\n",
       "      <td>2</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy birthday to my favorite gemini i love yo...</td>\n",
       "      <td>2</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ngo laureen thanks abi i love you most</td>\n",
       "      <td>2</td>\n",
       "      <td>4065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>narendramodi modiji very very very happy for y...</td>\n",
       "      <td>2</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>rt v basilio happy birthday simplyglyzz i miss...</td>\n",
       "      <td>2</td>\n",
       "      <td>6764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>shri shankarlalwani ji ko bahut bahut badhai a...</td>\n",
       "      <td>2</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>sanjayazadsln sanjay chor ye kaise sabit ho ra...</td>\n",
       "      <td>1</td>\n",
       "      <td>3236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>wow its really super world cup fananthem by ub...</td>\n",
       "      <td>2</td>\n",
       "      <td>3698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>rt arohayellowkies offclastro good morning lea...</td>\n",
       "      <td>2</td>\n",
       "      <td>7378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  index\n",
       "0       husne ylcnk romagnetique eloraxiong i love you      1   3647\n",
       "1    anuview badhai badhai badhai ji haan you are u...      2   1849\n",
       "2    happy birthday to my favorite gemini i love yo...      2    417\n",
       "3               ngo laureen thanks abi i love you most      2   4065\n",
       "4    narendramodi modiji very very very happy for y...      2    587\n",
       "..                                                 ...    ...    ...\n",
       "195  rt v basilio happy birthday simplyglyzz i miss...      2   6764\n",
       "196  shri shankarlalwani ji ko bahut bahut badhai a...      2    488\n",
       "197  sanjayazadsln sanjay chor ye kaise sabit ho ra...      1   3236\n",
       "198  wow its really super world cup fananthem by ub...      2   3698\n",
       "199  rt arohayellowkies offclastro good morning lea...      2   7378\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_200 = ordered_label_errors[:200]\n",
    "pd.DataFrame({\"text\": X_train[most_likely_200], \"label\" : y_train[most_likely_200], \"index\" : most_likely_200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
