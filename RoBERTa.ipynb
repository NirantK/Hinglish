{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa.ipynb",
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "mount_file_id": "1Xtd0C11wkcMSpTd_6bm5HXQTHDy-UDXQ",
      "authorship_tag": "ABX9TyM2H3PsHAcAU2I1ltA9IiRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/RoBERTa/RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liadde_Nq2t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install cleantext\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# !pip install tqdm --upgrade --force"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhwloxTFju5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HcaP8M6g9k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# %cd ..\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uSdzqqsVMRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp drive/My\\ Drive/Hinglish/big/lm_data.txt hinglish/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-3umcKorFd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "import cleantext\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "tqdm.pandas()\n",
        "data_folder = Path(\"drive/My Drive/Hinglish/big\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzVJDm_7pXpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data = open('hinglish/lm_data.txt').readlines()\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=1)\n",
        "len(train), len(test), len(valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdSPnnEnqENn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean(df, col):\n",
        "    \"\"\"Cleaning Twitter data\n",
        "    \n",
        "    Arguments:\n",
        "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\n",
        "        col {[string]} -- column in which text is present\n",
        "    \n",
        "    Returns:\n",
        "        [pandas dataframe] -- Datframe with a \"clean_text\" column\n",
        "    \"\"\"\n",
        "    df[\"clean_text\"] = df[col]\n",
        "    df[\"clean_text\"] = (\n",
        "        (df[\"clean_text\"])\n",
        "        .progress_apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"\", text))  # Removes RTS\n",
        "        .progress_apply(\n",
        "            lambda text: re.sub(r\"@\\w+ ?\", \"\", text)\n",
        "        )  # Replaces @ with mention\n",
        "        .progress_apply(lambda text: re.sub(r\"RT\", \"\", text))  # Replaces @ with mention\n",
        "        .progress_apply(\n",
        "            lambda text: re.sub(r\"#\\w+ ?\", \"\", text)\n",
        "        )  # Replaces # with hastag\n",
        "        .progress_apply(lambda text: re.sub(r\"http\\S+\", \"\", text))  # Removes URL\n",
        "    )\n",
        "    df[\"clean_text\"] = df[\"clean_text\"].progress_apply(\n",
        "        lambda x: cleantext.clean(x, all=True)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "toy = pd.DataFrame([\"RT @meghana https://something hello\"], columns=[\"text\"])\n",
        "clean(toy, \"text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB3lwgYSyaMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(data):\n",
        "    df = clean(pd.DataFrame(data, columns=[\"text\"]), \"text\")\n",
        "    return list(df['clean_text'])\n",
        "    \n",
        "train = clean_text(train)\n",
        "test = clean_text(test)\n",
        "valid = clean_text(valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsstbRAgyuaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[:5], test[:5], valid[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUMDFulZqPUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(f'hinglish/lm_data.valid.txt', 'w') as f:\n",
        "    for item in valid:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M05Ok1FF8yqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp hinglish/lm_data.train.txt  ../drive/My\\ Drive/Hinglish/ \n",
        "!cp hinglish/lm_data.test.txt  ../drive/My\\ Drive/Hinglish/ \n",
        "!cp hinglish/lm_data.valid.txt  ../drive/My\\ Drive/Hinglish/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcp1fd39XJY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pdb \n",
        "# !mkdir -p gpt2_bpe\n",
        "# !wget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
        "# !wget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
        "!for SPLIT in train valid test; do python -m examples.roberta.multiprocessing_bpe_encoder --encoder-json gpt2_bpe/encoder.json --vocab-bpe gpt2_bpe/vocab.bpe --inputs hinglish/lm_data.${SPLIT}.txt --outputs hinglish/lm_data.${SPLIT}.bpe --keep-empty --workers 60; done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCuft697Xlrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
        "!fairseq-preprocess --only-source --srcdict gpt2_bpe/dict.txt --trainpref hinglish/lm_data.train.bpe --validpref hinglish/lm_data.valid.bpe --testpref hinglish/lm_data.test.bpe --destdir data-bin/hinglish --workers 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COFsR9W_Xw7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fairseq-train  data-bin/hinglish --task masked_lm --criterion masked_lm --arch roberta_base --sample-break-mode complete --tokens-per-sample 512 --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 --lr-scheduler polynomial_decay --lr 0.0005 --warmup-updates 10000 --total-num-update 125000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 --max-sentences 16 --update-freq 16 --max-update 125000 --log-format simple --log-interval 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjkIth8eir8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "roberta = RobertaModel.from_pretrained('checkpoints', 'checkpoint_best.pt')\n",
        "assert isinstance(roberta.model, torch.nn.Module)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_tajt87uPdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}