{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combines all the results from all the experiments involving transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!git clone https://github.com/NirantK/Hinglish.git\n",
    "%cd Hinglish\n",
    "from hinglishutils import get_files_from_gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-Ki6v1a1jF79qx22gM6JlX1NVD4txTdn/view?usp=sharing\", \n",
    "                      \"train_lm.txt\")\n",
    "\n",
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-MRU7w2_la36qopO8Ob4BoCynOAZc0sZ/view?usp=sharing\", \n",
    "                      \"dev_lm.txt\")\n",
    "\n",
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-NqiU-tL5hW59MFtUXh1exivRokZKfs7/view?usp=sharing\", \n",
    "                      \"test_lm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1k4N0JlVOP-crIcCtC6ZI5Va8X3s2-r_D/view?usp=sharing\", \n",
    "                      \"test_labels_hinglish.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-FykBMdD7erRhr9370thtySNm6QvnQAA/view?usp=sharing\", \n",
    "                      \"train.json\")\n",
    "\n",
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-F6o4lSub2D-_iCoNPvxxnCiPQ82VJjG/view?usp=sharing\", \n",
    "                      \"test.json\")\n",
    "\n",
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-Esp4UtIZwX44eI8qndngweKZ6p9GLKT/view?usp=sharing\", \n",
    "                      \"valid.json\")\n",
    "\n",
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1bZ-cLeWkMBodvyqetCZR-TO9OdEc9R3y/view?usp=sharing\", \n",
    "                      \"final_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1-0bVrbhQ3nJhwmgIdhuL-ws4V9zuFpMF/view?usp=sharing\", \n",
    "                      \"hinglishBert.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1I1JXDg8ZzuuzXMN1X986oCeOjSxqZj7C/view?usp=sharing\", \n",
    "                      \"hinglishDistilBert.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files_from_gdrive(\"https://drive.google.com/file/d/1TTJzXi0dWYHVCrZM8vWoZzKWPfIF0ErB/view?usp=sharing\", \n",
    "                      \"hinglishRoberta.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling \n",
    "The run_language_modeling.py is taken from [huggingface/transformers/examples/language-modeling](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) \n",
    "\n",
    "- output_dir - directory to save your model files\n",
    "- model_type - Type of transfomer model bert/distilbert/roberta\n",
    "- model_name_or_path - name of the model. Model names can be found [here(official and community-uploaded)](https://huggingface.co/models) and [here(official)](https://huggingface.co/transformers/pretrained_models.html)\n",
    "- do_train\n",
    "- do_eval\n",
    "- train_data_file - path to train data text file\n",
    "- eval_data_file - path to eval data text file\n",
    "- mlm - This is used for BERT-based transforers. This stands for masked LMs  \n",
    "- num_train_epochs - Number of training epochs\n",
    "- save_total_limit - This saves the latest n checkpoints where n is the integer you provide to this parameter. We used 2 here because of size constrains on colab\n",
    "- overwrite_output_dir - overwrites the output folder that already exists/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py --output_dir=bert --model_type=bert --model_name_or_path=bert-base-multilingual-cased --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py --output_dir=distilbert --model_type=distilbert --model_name_or_path=distilbert-base-cased --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_language_modeling.py --output_dir=roberta --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HinglishBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hinglish import HinglishTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"bert\",\n",
    "    batch_size=16,\n",
    "    attention_probs_dropout_prob=0.4,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    learning_rate=5e-07,\n",
    "    adam_epsilon=1e-08\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HinglishDistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"distilbert\",\n",
    "    batch_size=16,\n",
    "    attention_probs_dropout_prob=0.6,\n",
    "    hidden_dropout_prob=0.6,\n",
    "    learning_rate=3e-05,\n",
    "    adam_epsilon=1e-08,\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HinglishRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"roberta\",\n",
    "    batch_size=16,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    learning_rate=4e-05,\n",
    "    adam_epsilon=5e-08,\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"distilbert\",\n",
    "    batch_size=16,\n",
    "    attention_probs_dropout_prob=0.8,\n",
    "    hidden_dropout_prob=0.4,\n",
    "    learning_rate=3.02e-05,\n",
    "    adam_epsilon=9.35e-05,\n",
    "    model_name=\"DB1\",\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"distilbert\",\n",
    "    batch_size=4,\n",
    "    attention_probs_dropout_prob=0.6,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    learning_rate=5.13e-05,\n",
    "    adam_epsilon=9.72e-05\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"distilbert\",\n",
    "    batch_size=16,\n",
    "    attention_probs_dropout_prob=0.4,\n",
    "    hidden_dropout_prob=0.6,\n",
    "    learning_rate=4.74e-05,\n",
    "    adam_epsilon=4.09e-05\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"bert\",\n",
    "    batch_size=4,\n",
    "    attention_probs_dropout_prob=0.6,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    learning_rate=5.13e-05,\n",
    "    adam_epsilon=9.72e-05\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinglishbert = HinglishTrainer(\n",
    "    \"bert\",\n",
    "    batch_size=4,\n",
    "    attention_probs_dropout_prob=0.7,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    learning_rate=5.01e-05,\n",
    "    adam_epsilon=4.79e-05\n",
    ")\n",
    "hinglishbert.train()\n",
    "hinglishbert.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
