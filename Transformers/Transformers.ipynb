{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h9LNCDTuWBD"
      },
      "source": [
        "# Combines all the results from all the experiments involving transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J61s_v7ouWBF"
      },
      "source": [
        "## Download all the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4AGtiCuWBH"
      },
      "source": [
        "!pip install transformers==3.3.1\n",
        "!pip install fastcore==1.0.13\n",
        "!pip install wandb==0.10.5\n",
        "!git clone https://github.com/NirantK/Hinglish.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WFKLR2AwZau"
      },
      "source": [
        "%cd Hinglish\n",
        "!git checkout origin/wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjOFdLQXxU3n"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IXyEoknwgZq"
      },
      "source": [
        "# Optional: log both gradients and parameters\n",
        "%env WANDB_WATCH=all\n",
        "%env WANDB_PROJECT=\"hinglish\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_0TozPvwfre"
      },
      "source": [
        "from hinglishutils import get_files_from_gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEXMpq5KuWBO"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-Ki6v1a1jF79qx22gM6JlX1NVD4txTdn/view?usp=sharing\", \n",
        "                      \"train_lm.txt\")\n",
        "\n",
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-MRU7w2_la36qopO8Ob4BoCynOAZc0sZ/view?usp=sharing\", \n",
        "                      \"dev_lm.txt\")\n",
        "\n",
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-NqiU-tL5hW59MFtUXh1exivRokZKfs7/view?usp=sharing\", \n",
        "                      \"test_lm.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzSM9vMEuWBS"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1k4N0JlVOP-crIcCtC6ZI5Va8X3s2-r_D/view?usp=sharing\", \n",
        "                      \"test_labels_hinglish.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VPEKb36uWBX"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-FykBMdD7erRhr9370thtySNm6QvnQAA/view?usp=sharing\", \n",
        "                      \"train.json\")\n",
        "\n",
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-F6o4lSub2D-_iCoNPvxxnCiPQ82VJjG/view?usp=sharing\", \n",
        "                      \"test.json\")\n",
        "\n",
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-Esp4UtIZwX44eI8qndngweKZ6p9GLKT/view?usp=sharing\", \n",
        "                      \"valid.json\")\n",
        "\n",
        "get_files_from_gdrive(\"https://drive.google.com/file/d/17wFvtj9tfp4QI6FrErAyqL9H1s5-lZkR/view?usp=sharing\", \n",
        "                      \"final_test.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ntmK5AxuWBa"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1-0bVrbhQ3nJhwmgIdhuL-ws4V9zuFpMF/view?usp=sharing\", \n",
        "                      \"hinglishBert.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1gp1izuWBd"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1I1JXDg8ZzuuzXMN1X986oCeOjSxqZj7C/view?usp=sharing\", \n",
        "                      \"hinglishDistilBert.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0sMj2euWBf"
      },
      "source": [
        "get_files_from_gdrive(\"https://drive.google.com/file/d/1TTJzXi0dWYHVCrZM8vWoZzKWPfIF0ErB/view?usp=sharing\", \n",
        "                      \"hinglishRoberta.tar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZKzabiEuWBh"
      },
      "source": [
        "# Language Modelling \n",
        "The run_language_modeling.py is taken from [huggingface/transformers/examples/language-modeling](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) \n",
        "\n",
        "- output_dir - directory to save your model files\n",
        "- model_type - Type of transfomer model bert/distilbert/roberta\n",
        "- model_name_or_path - name of the model. Model names can be found [here(official and community-uploaded)](https://huggingface.co/models) and [here(official)](https://huggingface.co/transformers/pretrained_models.html)\n",
        "- do_train\n",
        "- do_eval\n",
        "- train_data_file - path to train data text file\n",
        "- eval_data_file - path to eval data text file\n",
        "- mlm - This is used for BERT-based transforers. This stands for masked LMs  \n",
        "- num_train_epochs - Number of training epochs\n",
        "- save_total_limit - This saves the latest n checkpoints where n is the integer you provide to this parameter. We used 2 here because of size constrains on colab\n",
        "- overwrite_output_dir - overwrites the output folder that already exists/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgx-FJbNuWBh"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=bert --model_type=bert --model_name_or_path=bert-base-multilingual-cased --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMnob3xNuWBj"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=distilbert --model_type=distilbert --model_name_or_path=distilbert-base-cased --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw4lqW9YuWBl"
      },
      "source": [
        "!python run_language_modeling.py --output_dir=roberta --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=train_lm_data.txt --do_eval --eval_data_file=dev_lm_data.txt --mlm  --num_train_epochs 3 --save_total_limit 2 --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d59tYrnyuWBm"
      },
      "source": [
        "# HinglishBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxyMUkBSuWBn"
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vGtEne1uWBp",
        "outputId": "e5f1de1f-f910-4f4e-88b1-d128340c4a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from hinglish import HinglishTrainer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0bd6a5c9cf2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhinglish\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHinglishTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Hinglish/hinglish.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Integrations: this needs to come before other ml imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# in order to allow any 3rd-party code to initialize properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .integrations import (  # isort:skip\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mis_comet_available\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mis_optuna_available\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# No ML framework or transformer imports above this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPREFIX_CHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestRun\u001b[0m  \u001b[0;31m# isort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m  \u001b[0;31m# isort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mapex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mamp\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0m_has_apex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apex/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m from apex.exceptions import (ApexAuthSecret,\n\u001b[1;32m     17\u001b[0m                              ApexSessionSecret)\n\u001b[0;32m---> 18\u001b[0;31m from apex.interfaces import (ApexImplementation,\n\u001b[0m\u001b[1;32m     19\u001b[0m                              IApex)\n\u001b[1;32m     20\u001b[0m from apex.lib.libapex import (groupfinder,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apex/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mApexImplementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \"\"\" Class so that we can tell if Apex is installed from other \n\u001b[1;32m     12\u001b[0m     \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apex/interfaces.py\u001b[0m in \u001b[0;36mApexImplementation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimplements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIApex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zope/interface/declarations.py\u001b[0m in \u001b[0;36mimplements\u001b[0;34m(*interfaces)\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# the coverage for this block there. :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mPYTHON3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ADVICE_ERROR\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'implementer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0m_implements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"implements\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassImplements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Class advice impossible in Python3.  Use the @implementer class decorator instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zgIZou9uWBr",
        "outputId": "d1580008-a94a-46fb-e739-aca7c88e6deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"bert\",\n",
        "    batch_size=16,\n",
        "    attention_probs_dropout_prob=0.4,\n",
        "    hidden_dropout_prob=0.3,\n",
        "    learning_rate=5e-07,\n",
        "    adam_epsilon=1e-08\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-192d1609152b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hinglishbert = HinglishTrainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mattention_probs_dropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HinglishTrainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKcNA0OeuWBs"
      },
      "source": [
        "# HinglishDistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwR7VW8RuWBt"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"distilbert\",\n",
        "    batch_size=16,\n",
        "    attention_probs_dropout_prob=0.6,\n",
        "    hidden_dropout_prob=0.6,\n",
        "    learning_rate=3e-05,\n",
        "    adam_epsilon=1e-08,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChDX3lEzuWBu"
      },
      "source": [
        "# HinglishRoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HlTkk0QuWBv"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"roberta\",\n",
        "    batch_size=16,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    learning_rate=4e-05,\n",
        "    adam_epsilon=5e-08,\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECxeAdEWuWBw"
      },
      "source": [
        "# Ensemble Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WboNsL2uWBw"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"distilbert\",\n",
        "    batch_size=16,\n",
        "    attention_probs_dropout_prob=0.8,\n",
        "    hidden_dropout_prob=0.4,\n",
        "    learning_rate=3.02e-05,\n",
        "    adam_epsilon=9.35e-05,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdfHKZo9uWBx"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"distilbert\",\n",
        "    batch_size=4,\n",
        "    attention_probs_dropout_prob=0.6,\n",
        "    hidden_dropout_prob=0.2,\n",
        "    learning_rate=5.13e-05,\n",
        "    adam_epsilon=9.72e-05,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuweXbRmuWBz"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"distilbert\",\n",
        "    batch_size=16,\n",
        "    attention_probs_dropout_prob=0.4,\n",
        "    hidden_dropout_prob=0.6,\n",
        "    learning_rate=4.74e-05,\n",
        "    adam_epsilon=4.09e-05,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KploUFENuWB0"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"bert\",\n",
        "    batch_size=4,\n",
        "    attention_probs_dropout_prob=0.6,\n",
        "    hidden_dropout_prob=0.2,\n",
        "    learning_rate=5.13e-05,\n",
        "    adam_epsilon=9.72e-05,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE2tgL7-uWB2"
      },
      "source": [
        "hinglishbert = HinglishTrainer(\n",
        "    \"bert\",\n",
        "    batch_size=4,\n",
        "    attention_probs_dropout_prob=0.7,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    learning_rate=5.01e-05,\n",
        "    adam_epsilon=4.79e-05,\n",
        "    lm_model_dir=\"distilBert6\"\n",
        ")\n",
        "hinglishbert.train()\n",
        "hinglishbert.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}