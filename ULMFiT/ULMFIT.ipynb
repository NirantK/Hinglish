{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/utils/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs \n",
    "Restart the runtime after all the installs are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ninja\n",
    "# !pip install --upgrade --force-reinstall fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install cleantext\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm --upgrade --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Loading and Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from hinglishutils import get_files_from_gdrive\n",
    "from pathlib import Path\n",
    "datapath = Path(\"../data\")\n",
    "data_raw = datapath/\"raw\"\n",
    "data_interim = datapath/\"interim\"\n",
    "data_processed = datapath/\"processed\"\n",
    "cleanlab_datapath = datapath/\"cleanlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from datetime import datetime\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "import cleantext\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gdown\n",
    "\n",
    "tqdm.pandas()\n",
    "data_folder = Path(\"drive/My Drive/Hinglish/big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(data_folder / \"hinglish_sp.model\"))\n",
    "\n",
    "\n",
    "class SpTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang: str, vocab_size: int = 8000):\n",
    "        self.lang = lang\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(str(data_folder / \"hinglish_sp.model\"))\n",
    "        self.vocab = Vocab([self.sp.IdToPiece(int(i)) for i in range(self.vocab_size)])\n",
    "\n",
    "    def tokenizer(self, t: str) -> List[str]:\n",
    "        return self.sp.EncodeAsPieces(t)\n",
    "\n",
    "    def detokenizer(self, t: List[str]) -> str:\n",
    "        return self.sp.DecodePieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, col):\n",
    "    \"\"\"Cleaning Twitter data\n",
    "    \n",
    "    Arguments:\n",
    "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\n",
    "        col {[string]} -- column in which text is present\n",
    "    \n",
    "    Returns:\n",
    "        [pandas dataframe] -- Datframe with a \"clean_text\" column\n",
    "    \"\"\"\n",
    "    df[\"clean_text\"] = df[col]\n",
    "    df[\"clean_text\"] = (\n",
    "        (df[\"clean_text\"])\n",
    "        .progress_apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"\", text))  # Removes RTS\n",
    "        .progress_apply(\n",
    "            lambda text: re.sub(r\"@\\w+ ?\", \"\", text)\n",
    "        )  # Replaces @ with mention\n",
    "        .progress_apply(lambda text: re.sub(r\"RT\", \"\", text))  # Replaces @ with mention\n",
    "        .progress_apply(\n",
    "            lambda text: re.sub(r\"#\\w+ ?\", \"\", text)\n",
    "        )  # Replaces # with hastag\n",
    "        .progress_apply(lambda text: re.sub(r\"http\\S+\", \"\", text))  # Removes URL\n",
    "    )\n",
    "    df[\"clean_text\"] = df[\"clean_text\"].progress_apply(\n",
    "        lambda x: cleantext.clean(x, all=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "toy = pd.DataFrame([\"RT @meghana https://something hello\"], columns=[\"text\"])\n",
    "clean(toy, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_files():\n",
    "    test = pd.read_json(\"drive/My Drive/Hinglish/interim/test.json\")\n",
    "    train = pd.read_json(\"drive/My Drive/Hinglish/interim/train.json\")\n",
    "    valid = pd.read_json(\"drive/My Drive/Hinglish/interim/valid.json\")\n",
    "    final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
    "    hinglish_unsup_high_confidence = pd.read_json(\n",
    "        \"drive/My Drive/Hinglish/interim/hinglish_unsup_high_confidence.json\"\n",
    "    )\n",
    "    hinglish_unsup_less_confidence = pd.read_json(\n",
    "        \"drive/My Drive/Hinglish/interim/hinglish_unsup_less_confidence.json\"\n",
    "    )\n",
    "    test = clean(test, \"text\")\n",
    "    train = clean(train, \"text\")\n",
    "    valid = clean(valid, \"text\")\n",
    "    final_test = clean(final_test, \"text\")\n",
    "    hinglish_unsup_high_confidence = clean(hinglish_unsup_high_confidence, 0)\n",
    "    hinglish_unsup_less_confidence = clean(hinglish_unsup_less_confidence, 0)\n",
    "    return (\n",
    "        train,\n",
    "        test,\n",
    "        valid,\n",
    "        final_test,\n",
    "        hinglish_unsup_high_confidence,\n",
    "        hinglish_unsup_less_confidence,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unstructured_hinglish_from_txt():\n",
    "    data = open(\"big_unstruct.txt\", \"r\").readlines()\n",
    "    data += open(data_folder / \"train.txt\", \"r\").readlines()\n",
    "    data += open(data_folder / \"valid.txt\", \"r\").readlines()\n",
    "    df = clean(pd.DataFrame(data, columns=[\"text\"]), \"text\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_unstructured_hinglish_from_df(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    "):\n",
    "    data = list(load_unstructured_hinglish_from_txt()[\"clean_text\"])\n",
    "    data += list(train[\"clean_text\"])\n",
    "    data += list(test[\"clean_text\"])\n",
    "    data += list(valid[\"clean_text\"])\n",
    "    data += list(final_test[\"clean_text\"])\n",
    "    data += list(hinglish_unsup_high_confidence[0])\n",
    "    data += list(hinglish_unsup_less_confidence[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    learn,\n",
    "    model_name: str,\n",
    "    lr=2e-03,\n",
    "    loops=5,\n",
    "    num_epocs_per_loop=2,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    "):\n",
    "    learn.fit_one_cycle(1, slice(lr))\n",
    "    learn.unfreeze()\n",
    "    if test:\n",
    "        learn.save(f\"{model_name}_test\")\n",
    "        return\n",
    "    for i in range(loops):\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(num_epocs_per_loop, slice(lr))\n",
    "        learn.save(f\"{model_name}_{i}\")\n",
    "        if encoder:\n",
    "            learn.save_encoder(f\"{model_name}_enc_{i}\")\n",
    "        print(f\"saved {model_name}_{i}\")\n",
    "        learn.recorder.plot_losses()\n",
    "        learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Creating databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_unstruct_id = \"1TcUKflmq4nAV2-YopYz1VqyEG9xrVIc1\"\n",
    "get_files_from_gdrive(fname=\"big_unstruct.txt\", file_id=big_unstruct_id)\n",
    "tokenizer = Tokenizer(tok_func=SpTokenizer)\n",
    "(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    ") = load_and_clean_files()\n",
    "data = load_unstructured_hinglish_from_df(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    ")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"clean_text\"])\n",
    "train_lm, valid_lm = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=train_lm,\n",
    "    valid_df=valid_lm,\n",
    "    text_cols=\"clean_text\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"done\")\n",
    "data_lm.save(\"clean_lm.\" + \"pkl\")\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls_lm = TextLMDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=pd.concat([train, final_test, test]),\n",
    "    valid_df=valid,\n",
    "    text_cols=\"clean_text\",\n",
    "    label_cols=\"sentiment\",\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=data_lm.vocab,\n",
    ")\n",
    "print(\"done\")\n",
    "data_cls_lm.save(\"clean_cls_lm.\" + \"pkl\")\n",
    "data_cls_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = TextClasDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=pd.concat([train, test]),\n",
    "    valid_df=valid,\n",
    "    text_cols=\"clean_text\",\n",
    "    label_cols=\"sentiment\",\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=data_lm.vocab,\n",
    ")\n",
    "print(\"done\")\n",
    "data_cls.save(\"clean_cls.\" + \"pkl\")\n",
    "data_cls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path=data_folder, file=\"clean_lm.pkl\")\n",
    "data_cls_lm = load_data(path=data_folder, file=\"clean_cls_lm.pkl\")\n",
    "data_cls = load_data(path=data_folder, file=\"clean_cls.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm_lm_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.25,\n",
    "    hidden_p=0.1,\n",
    "    input_p=0.2,\n",
    "    embed_p=0.02,\n",
    "    weight_p=0.15,\n",
    "    tie_weights=True,\n",
    "    out_bias=True,\n",
    ")\n",
    "learn = language_model_learner(\n",
    "    data_lm,\n",
    "    arch=AWD_LSTM,\n",
    "    config=awd_lstm_lm_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy, Perplexity()],\n",
    "    pretrained=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"language_model\",\n",
    "    lr=2e-03,\n",
    "    loops=5,\n",
    "    num_epocs_per_loop=2,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LM Encoder with Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm_lm_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.25,\n",
    "    hidden_p=0.1,\n",
    "    input_p=0.2,\n",
    "    embed_p=0.02,\n",
    "    weight_p=0.15,\n",
    "    tie_weights=True,\n",
    "    out_bias=True,\n",
    ")\n",
    "learn = language_model_learner(\n",
    "    data_cls_lm, arch=AWD_LSTM, config=awd_lstm_lm_config, pretrained=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "learn.load(f\"language_model_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"language_model_class\",\n",
    "    lr=2e-03,\n",
    "    loops=10,\n",
    "    num_epocs_per_loop=10,\n",
    "    encoder=True,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm_clas_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.4,\n",
    "    hidden_p=0.2,\n",
    "    input_p=0.6,\n",
    "    embed_p=0.1,\n",
    "    weight_p=0.5,\n",
    ")\n",
    "learn = text_classifier_learner(\n",
    "    data_cls,\n",
    "    AWD_LSTM,\n",
    "    config=awd_lstm_clas_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy],\n",
    "    pretrained=False,\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "learn.load_encoder(f\"language_model_class_enc_{i}\")\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"class_model\",\n",
    "    lr=2e-03,\n",
    "    loops=10,\n",
    "    num_epocs_per_loop=10,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = load_data(path=data_folder, file=\"clean_cls.pkl\")\n",
    "final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
    "final_test = clean(final_test, \"text\")\n",
    "awd_lstm_clas_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.4,\n",
    "    hidden_p=0.2,\n",
    "    input_p=0.6,\n",
    "    embed_p=0.1,\n",
    "    weight_p=0.5,\n",
    ")\n",
    "learn = text_classifier_learner(\n",
    "    data_cls,\n",
    "    AWD_LSTM,\n",
    "    config=awd_lstm_clas_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy],\n",
    "    pretrained=False,\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "learn.load(f\"test_class_model_{i}\")\n",
    "final_test[\"predicted\"] = final_test[\"clean_text\"].progress_apply(\n",
    "    lambda x: str(learn.predict(x)[0])\n",
    ")\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.to_csv(data_folder / \"answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder / \"answer.txt\", \"w\") as f:\n",
    "    f.write(\"Uid, Sentiment\\n\")\n",
    "\n",
    "with open(data_folder / \"answer.txt\", \"a\") as f:\n",
    "    for i in range(len(final_test[\"predicted\"].tolist())):\n",
    "        f.write(f\"{final_test.loc[i]['uid']},{final_test.loc[i]['predicted']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(data_folder / \"answer.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
