{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"outputs\").resolve()\n",
    "filenames = [file for file in data_path.ls() if file.suffix == \".jsonl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt: str):\n",
    "    txt = re.sub(r\"RT\\s@\\w+:\", \"\", txt)  # Removes RTS\n",
    "    txt = re.sub(r\"@\", \" \", txt)  # Replaces @ with mention\n",
    "    txt = re.sub(r\"#\", \" \", txt)  # Replaces # with hastag\n",
    "    txt = re.sub(r\"http\\S+\", \"\", txt)  # Removes URL\n",
    "    txt = re.sub(r\"// .*$\", \":\", txt)\n",
    "    return re.sub(r\"\\W+\", \" \", txt)\n",
    "\n",
    "\n",
    "clean(\"asdasd!!! #{75}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(txt: str):\n",
    "    return txt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids, text = [], []\n",
    "for file in filenames:\n",
    "    with file.open(\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        texts = [json.loads(line)[\"text\"] for line in lines]\n",
    "        texts = [list(set(tokenizer(clean(txt)))) for txt in texts]\n",
    "        text.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [item for sublist in text for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_cnt = Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_cnt.most_common(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
