{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/cleanlab/CleanlabDistilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is cleanlab?\n",
    "\n",
    "<br /> \n",
    "\n",
    "[cleanlab](https://l7.curtisnorthcutt.com/cleanlab-python-package) is a machine learning python package for learning with noisy labels and finding label errors in datasets. cleanlab CLEANs LABels. It is powered by the theory of confident learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "from hinglishutils import get_files_from_gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "datapath = Path(\"../data\")\n",
    "data_raw = datapath/\"raw\"\n",
    "data_interim = datapath/\"interim\"\n",
    "data_processed = datapath/\"processed\"\n",
    "cleanlab_datapath = datapath/\"cleanlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    \"https://drive.google.com/file/d/1T6N_ba6-w2xLM6t4_EINGR7SGVQpCXd_/view?usp=sharing\"\n",
    ")\n",
    "\n",
    "get_files_from_gdrive(url, str(data_raw/\"distilBertOutput.csv\"))\n",
    "DistilBert = pd.read_csv(data_raw/\"distilBertOutput.csv\")\n",
    "DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = {\"neutral\": 0, \"negative\": 1, \"positive\": 2}\n",
    "DistilBert[\"labels\"] = DistilBert.Sentiment.map(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(DistilBert[\"clean_text\"])\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sents = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`py_train` -> What is the percentage of each labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_train = cleanlab.util.value_counts(DistilBert[\"labels\"]) / float(\n",
    "    len(DistilBert[\"labels\"])\n",
    ")\n",
    "py_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_method = \"prune_by_noise_rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(list(DistilBert[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "\n",
    "(jc, psx) = cleanlab.latent_estimation.estimate_confident_joint_and_cv_pred_proba(\n",
    "    sents,\n",
    "    np.array(list(DistilBert[\"labels\"])),\n",
    "    clf=LogReg(multi_class=\"auto\", solver=\"lbfgs\", max_iter=10000, verbose=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_py, est_nm, est_inv = cleanlab.latent_estimation.estimate_latent(\n",
    "    jc, np.array(list(DistilBert[\"labels\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered = cleanlab.pruning.get_noise_indices(\n",
    "    np.array(list(DistilBert[\"labels\"])),\n",
    "    psx,\n",
    "    #    est_inv,\n",
    "    prune_method=prune_method,\n",
    "    frac_noise=0.37,\n",
    "    sorted_index_method=\"normalized_margin\",\n",
    ")\n",
    "print(\"Number of estimated errors in test set:\", len(ordered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "errors = []\n",
    "for idx in ordered[:100]:\n",
    "    errors.append(\n",
    "        f\"Estimated Error in {idx}\\npredicted {DistilBert['Sentiment'][idx]}\\n sentence {DistilBert['clean_text'][idx]}\\n -----\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleanlab_datapath/'errorsDistilBert.txt', 'w') as f: \n",
    "    for item in errors:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
