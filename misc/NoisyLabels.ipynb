{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating the Labels as Noisy\n",
    "### Learning from Noisy Labels using CleanLab\n",
    "\n",
    "Link: https://github.com/cgnorthcutt/cleanlab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "datapath = Path(\"../data\")\n",
    "data_raw = datapath/\"raw\"\n",
    "data_interim = datapath/\"interim\"\n",
    "data_processed = datapath/\"processed\"\n",
    "cleanlab_datapath = datapath/\"cleanlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(data_interim/'train-large.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(list(train['sentiment']))); num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refering to : [Twitter-Airlines](https://github.com/martinpella/twitter-airlines/blob/master/shallow_learning.ipynb) for cleaning data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade snowballstemmer\n",
    "# !pip install --upgrade nltk\n",
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TextCleaner, CleanTwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_train, X_test, y_train, y_test, tfidf_train, tfidf_test = CleanTwitter(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningWithNoisyLabels : [IrisSimple](https://github.com/cgnorthcutt/cleanlab/blob/master/examples/iris_simple_example.ipynb)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "from cleanlab.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.noise_generation import generate_noisy_labels\n",
    "from cleanlab.util import value_counts\n",
    "from cleanlab.latent_algebra import compute_inv_noise_matrix\n",
    "import cleanlab\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "np.random.seed(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if this is how you do it? HOW TO CHANGE THESE VALUES?\n",
    "\n",
    "# Set the sparsity of the noise matrix.\n",
    "FRAC_ZERO_NOISE_RATES = 0.5\n",
    "# A proxy for the fraction of labels that are correct.\n",
    "avg_trace = 0.67  # ~33% wrong labels. Increasing makes the problem easier.\n",
    "\n",
    "p_y = value_counts(y_train)  # probability of y estimates\n",
    "noise_matrix = generate_noise_matrix_from_trace(\n",
    "    K=num_classes,\n",
    "    trace=num_classes * avg_trace,\n",
    "    py=p_y,\n",
    "    frac_zero_noise_rates=FRAC_ZERO_NOISE_RATES,\n",
    ")\n",
    "\n",
    "# Create noisy labels\n",
    "s = generate_noisy_labels(y_train, noise_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITHOUT confident learning,\", end=\" \")\n",
    "m = LogisticRegression(\n",
    "    C=4, dual=False, multi_class=\"auto\", solver=\"lbfgs\", max_iter=1000\n",
    ")\n",
    "_ = m.fit(tfidf_train, y_train)\n",
    "pred = m.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH confident learning (without noise matrix given),\", end=\" \")\n",
    "m_rp = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m_rp)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH confident learning (noise matrix given),\", end=\" \")\n",
    "m2_rp = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m2_rp)\n",
    "_ = rp.fit(tfidf_train, y_train, noise_matrix=noise_matrix)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH confident learning (noise / inverse noise matrix given),\", end=\" \")\n",
    "m3_rp = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m3_rp)\n",
    "_ = rp.fit(\n",
    "    tfidf_train,\n",
    "    y_train,\n",
    "    noise_matrix=noise_matrix,\n",
    "    inverse_noise_matrix=compute_inv_noise_matrix(p_y, noise_matrix),\n",
    ")\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH confident learning (using latent noise matrix estimation),\", end=\" \")\n",
    "m = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WITH confident learning (using calibrated confident joint),', end=\" \")\n",
    "m = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
    "rp = LearningWithNoisyLabels(clf=m)\n",
    "_ = rp.fit(tfidf_train, y_train)\n",
    "pred = rp.predict(tfidf_test)\n",
    "print(\"test accuracy:\", round(accuracy_score(pred, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s != y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade hypopt # this is not a typo, hypopt and hyperopt are different packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hypopt.model_selection import GridSearch\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"prune_method\": [\"prune_by_noise_rate\", \"prune_by_class\", \"both\"],\n",
    "    \"converge_latent_estimates\": [True, False],\n",
    "}\n",
    "# Fit LearningWithNoisyLabels across all parameter settings.\n",
    "params = ParameterGrid(param_grid)\n",
    "scores = []\n",
    "for param in params:\n",
    "    clf = LogisticRegression(solver = 'lbfgs', multi_class = 'auto', max_iter = 1000)\n",
    "    rp = LearningWithNoisyLabels(clf = clf, **param)\n",
    "    _ = rp.fit(tfidf_train, s) # s is the noisy y_train labels\n",
    "    scores.append(accuracy_score(rp.predict(tfidf_test), y_test))\n",
    "\n",
    "# Print results sorted from best to least\n",
    "for i in np.argsort(scores)[::-1]:\n",
    "    print(\"Param settings:\", params[i])\n",
    "    print(\n",
    "        \"Hinglish dataset test accuracy (using confident learning):\\t\", \n",
    "        round(scores[i], 5),\n",
    "        \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 Most Likely Wrong Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/cgnorthcutt/rankpruning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psx = cleanlab.latent_estimation.estimate_cv_predicted_probabilities(\n",
    "    tfidf_train,\n",
    "    s,\n",
    "    clf=LogisticRegression(max_iter=1000, multi_class=\"auto\", solver=\"lbfgs\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.asarray(s)\n",
    "psx = np.asarray(psx)\n",
    "pd.set_option(\"display.max_colwidth\", 201)\n",
    "pd.set_option(\"max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "import cleanlab\n",
    "\n",
    "jc, psx = cleanlab.latent_estimation.estimate_confident_joint_and_cv_pred_proba(\n",
    "    tfidf_train, y_train, rp\n",
    ")\n",
    "\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s=s, psx=psx, sorted_index_method=\"normalized_margin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_label_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_200 = ordered_label_errors[:200]\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"text\": X_train[most_likely_200],\n",
    "        \"label\": y_train[most_likely_200],\n",
    "        \"index\": most_likely_200,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
