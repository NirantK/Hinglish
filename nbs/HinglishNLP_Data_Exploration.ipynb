{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/radhikasethi2011/Hinglish/blob/data_exploration/HinglishNLP_Data_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CleanClassFiles = json files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive/CleanClassFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "p = Path('.')\n",
    "allFiles = []\n",
    "for json_file in p.iterdir():\n",
    "    allFiles.append(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = [] \n",
    "for json_file in allFiles:\n",
    "  input_file = open(json_file,'r')\n",
    "  f = open(json_file)\n",
    "  decoded_text = json.load(f)\n",
    "  for item in decoded_text:\n",
    "    text_data = item.get('clean_text')\n",
    "    print(text_data)\n",
    "    result.append(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/json_clean_text.txt', 'w') as outfile:\n",
    "  outfile.write(\"Text\\n\")\n",
    "  for r in result:\n",
    "    outfile.write(\"%s\\n\" % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning json_clean_text.txt data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "df = pd.read_csv('/content/gdrive/MyDrive/DataExploration/json_clean_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, col):\n",
    "    \"\"\"Cleaning Twiitter data\n",
    "    Arguments:\n",
    "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\n",
    "        col {[string]} -- column in which text is present\n",
    "    Returns:\n",
    "        [pandas dataframe] -- Datframe with a \"clean_text\" column\n",
    "    \"\"\"\n",
    "    df[\"clean_text\"] = df[col]\n",
    "    df[\"clean_text\"] = (\n",
    "        (df[\"clean_text\"])\n",
    "        .apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"Retweet\", text))  # Removes RTS\n",
    "        .apply(lambda text: re.sub(r\"@\", \"mention \", text))  # Replaces @ with mention\n",
    "        .apply(lambda text: re.sub(r\"#\", \"hashtag \", text))  # Replaces # with hastag\n",
    "        .apply(lambda text: re.sub(r\"http\\S+\", \"\", text))  # Removes URL\n",
    "        .apply(lambda text: re.sub(r'// .*$', \":\", text)\n",
    "        \n",
    "    )\n",
    "    return df\n",
    "\n",
    "df1 = clean(df, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving clean json text as jclean_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt(r'/content/gdrive/MyDrive/DataExploration/jclean_text.txt', df1['clean_text'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatinating jclean_text.txt with 3 text files in CleanLMFiles\n",
    "1. concat CleanLM Files \n",
    "2. Concat giga_CleanLM File with cleaned_json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/CleanLMFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('.')\n",
    "allFiles = []\n",
    "for text_file in p.iterdir():\n",
    "  allFiles.append(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/giga_text.txt', 'w') as outfile:\n",
    "  outfile.write(\"Text\\n\")\n",
    "  for f in allFiles:\n",
    "    with open(f) as infile:\n",
    "      contents = infile.read()\n",
    "      outfile.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "%cd /content/gdrive/MyDrive/DataExploration/\n",
    "\n",
    "p = Path('.')\n",
    "allFiles = []\n",
    "for text_file in p.iterdir():\n",
    "  if not text_file.is_dir():\n",
    "    allFiles.append(text_file)\n",
    "\n",
    "print(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/DataExploration/MegaTextDoc.txt', 'w') as outfile:\n",
    "  outfile.write(\"Text\\n\")\n",
    "  for f in allFiles:\n",
    "    with open(f) as infile:\n",
    "      contents = infile.read()\n",
    "      outfile.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mega file done. \n",
    "Data exploration! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of stop words from [HinglishNLP](https://github.com/TrigonaMinima/HinglishNLP/blob/master/data/assets/stop_hinglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/DataExploration/\n",
    "stopwords = []\n",
    "text_file = open('stopwords-hinglish.txt', 'r')\n",
    "stopwords = [line.split(\"\\n\")[0] for line in text_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords #stopwords list from hinglish-stopwords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/DataExploration/mega_text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('MegaTextDoc.txt', sep='delimiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#everything in lowertext \n",
    "df['Text'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of chars per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords) #set of the hinglish stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.update(['Retweet','retweet','mention',':','.','...','....', '//','-','/','!','?','hashtag', '_','â€¦','\"\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus \n",
    "corpus1 = []\n",
    "new= df['Text'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus1=[word for i in new for word in i]\n",
    "\n",
    "#dictionary of stop words\n",
    "from collections import defaultdict\n",
    "dic=defaultdict(int)\n",
    "for word in corpus1:\n",
    "    if word in stop:\n",
    "        dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new #list of lists of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop))\n",
    "print(len(corpus))\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the most common stopwords in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding words other than stopwords that occur frequently in the hinglish data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "counter=Counter(corpus1)\n",
    "most=counter.most_common()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most #most common words from data eliminating the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for most common words excluding stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y= [], []\n",
    "\n",
    "\n",
    "for word,count in most[:100]:\n",
    "  if (word not in stop):\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the top bi-grams in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_ngram(corpus1, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus1)\n",
    "    bag_of_words = vec.transform(corpus1)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the top tri-grams in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bi_grams=get_top_ngram(df['Text'],n=2)\n",
    "x,y=map(list,zip(*top_bi_grams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tri_grams=get_top_ngram(df['Text'],n=3)\n",
    "x,y=map(list,zip(*top_tri_grams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modelling \n",
    "- tokenize \n",
    "- removing stopwords \n",
    "- lemmatize \n",
    "- bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].fillna('').apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_data(df):\n",
    "    corpus=[]\n",
    "    stem=PorterStemmer()\n",
    "    lem=WordNetLemmatizer()\n",
    "    #and re.match('^[a-zA-Z]+', w)\n",
    "    for text in df['Text']:\n",
    "      words=[w for w in word_tokenize(text) if (w not in stop)]\n",
    "      words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "      corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus=preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words - gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "dic=gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Model \n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 4, id2word = dic, passes = 1, workers = 4)\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyLDAvis\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Analysis - on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vocabulary Size - Number of unique words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set(x for l in new for x in l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Top 100 words by frequency - removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stopwords \n",
    "counter1=Counter(corpus1)\n",
    "most1=counter1.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without stopwords \n",
    "dic1=defaultdict(int)\n",
    "for word in corpus1:\n",
    "    if word not in stop:\n",
    "        dic1[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 frequent words \n",
    "\n",
    "c = Counter(dic1)\n",
    "top_100 = c.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of top 10 words in the data, excluding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=sorted(dic1.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=sorted(dic1.items(), key=lambda x:x[1],reverse=True)[:500]\n",
    "x,y=zip(*top)\n",
    "#plt.subplots(figsize=(18,10))\n",
    "plt.figure(figsize=(18,100)) \n",
    "plt.barh(x,y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
