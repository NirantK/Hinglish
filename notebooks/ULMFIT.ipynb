{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/utils/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoS-0gbuT0Hy"
   },
   "source": [
    "# Installs \n",
    "Restart the runtime after all the installs are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HNgydmSHL4r"
   },
   "outputs": [],
   "source": [
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRVjhbwFHVTq"
   },
   "outputs": [],
   "source": [
    "# !pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhtFW2aOHey7"
   },
   "outputs": [],
   "source": [
    "# !pip install ninja\n",
    "# !pip install --upgrade --force-reinstall fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_IDGQtnHhKT"
   },
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install cleantext\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E32VCaB-y1J_"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm --upgrade --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmMXuMF0T7AY"
   },
   "source": [
    "# Data Cleaning and Loading and Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n317G3WgUDhy"
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from datetime import datetime\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "import cleantext\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gdown\n",
    "\n",
    "tqdm.pandas()\n",
    "data_folder = Path(\"drive/My Drive/Hinglish/big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUVOtz8bIh0r"
   },
   "outputs": [],
   "source": [
    "def get_files_from_gdrive(fname: str, file_id: str) -> None:\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(url, fname, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2nxVwrFJcnb"
   },
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(data_folder / \"hinglish_sp.model\"))\n",
    "\n",
    "\n",
    "class SpTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang: str, vocab_size: int = 8000):\n",
    "        self.lang = lang\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(str(data_folder / \"hinglish_sp.model\"))\n",
    "        self.vocab = Vocab([self.sp.IdToPiece(int(i)) for i in range(self.vocab_size)])\n",
    "\n",
    "    def tokenizer(self, t: str) -> List[str]:\n",
    "        return self.sp.EncodeAsPieces(t)\n",
    "\n",
    "    def detokenizer(self, t: List[str]) -> str:\n",
    "        return self.sp.DecodePieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LByC_kMRJvg_"
   },
   "outputs": [],
   "source": [
    "def clean(df, col):\n",
    "    \"\"\"Cleaning Twitter data\n",
    "    \n",
    "    Arguments:\n",
    "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\n",
    "        col {[string]} -- column in which text is present\n",
    "    \n",
    "    Returns:\n",
    "        [pandas dataframe] -- Datframe with a \"clean_text\" column\n",
    "    \"\"\"\n",
    "    df[\"clean_text\"] = df[col]\n",
    "    df[\"clean_text\"] = (\n",
    "        (df[\"clean_text\"])\n",
    "        .progress_apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"\", text))  # Removes RTS\n",
    "        .progress_apply(\n",
    "            lambda text: re.sub(r\"@\\w+ ?\", \"\", text)\n",
    "        )  # Replaces @ with mention\n",
    "        .progress_apply(lambda text: re.sub(r\"RT\", \"\", text))  # Replaces @ with mention\n",
    "        .progress_apply(\n",
    "            lambda text: re.sub(r\"#\\w+ ?\", \"\", text)\n",
    "        )  # Replaces # with hastag\n",
    "        .progress_apply(lambda text: re.sub(r\"http\\S+\", \"\", text))  # Removes URL\n",
    "    )\n",
    "    df[\"clean_text\"] = df[\"clean_text\"].progress_apply(\n",
    "        lambda x: cleantext.clean(x, all=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "toy = pd.DataFrame([\"RT @meghana https://something hello\"], columns=[\"text\"])\n",
    "clean(toy, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMwEOvlB2myr"
   },
   "outputs": [],
   "source": [
    "def load_and_clean_files():\n",
    "    test = pd.read_json(\"drive/My Drive/Hinglish/interim/test.json\")\n",
    "    train = pd.read_json(\"drive/My Drive/Hinglish/interim/train.json\")\n",
    "    valid = pd.read_json(\"drive/My Drive/Hinglish/interim/valid.json\")\n",
    "    final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
    "    hinglish_unsup_high_confidence = pd.read_json(\n",
    "        \"drive/My Drive/Hinglish/interim/hinglish_unsup_high_confidence.json\"\n",
    "    )\n",
    "    hinglish_unsup_less_confidence = pd.read_json(\n",
    "        \"drive/My Drive/Hinglish/interim/hinglish_unsup_less_confidence.json\"\n",
    "    )\n",
    "    test = clean(test, \"text\")\n",
    "    train = clean(train, \"text\")\n",
    "    valid = clean(valid, \"text\")\n",
    "    final_test = clean(final_test, \"text\")\n",
    "    hinglish_unsup_high_confidence = clean(hinglish_unsup_high_confidence, 0)\n",
    "    hinglish_unsup_less_confidence = clean(hinglish_unsup_less_confidence, 0)\n",
    "    return (\n",
    "        train,\n",
    "        test,\n",
    "        valid,\n",
    "        final_test,\n",
    "        hinglish_unsup_high_confidence,\n",
    "        hinglish_unsup_less_confidence,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APVTHWLvKiZp"
   },
   "outputs": [],
   "source": [
    "def load_unstructured_hinglish_from_txt():\n",
    "    data = open(\"big_unstruct.txt\", \"r\").readlines()\n",
    "    data += open(data_folder / \"train.txt\", \"r\").readlines()\n",
    "    data += open(data_folder / \"valid.txt\", \"r\").readlines()\n",
    "    df = clean(pd.DataFrame(data, columns=[\"text\"]), \"text\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_unstructured_hinglish_from_df(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    "):\n",
    "    data = list(load_unstructured_hinglish_from_txt()[\"clean_text\"])\n",
    "    data += list(train[\"clean_text\"])\n",
    "    data += list(test[\"clean_text\"])\n",
    "    data += list(valid[\"clean_text\"])\n",
    "    data += list(final_test[\"clean_text\"])\n",
    "    data += list(hinglish_unsup_high_confidence[0])\n",
    "    data += list(hinglish_unsup_less_confidence[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCwRjR5833YZ"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    learn,\n",
    "    model_name: str,\n",
    "    lr=2e-03,\n",
    "    loops=5,\n",
    "    num_epocs_per_loop=2,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    "):\n",
    "    learn.fit_one_cycle(1, slice(lr))\n",
    "    learn.unfreeze()\n",
    "    if test:\n",
    "        learn.save(f\"{model_name}_test\")\n",
    "        return\n",
    "    for i in range(loops):\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(num_epocs_per_loop, slice(lr))\n",
    "        learn.save(f\"{model_name}_{i}\")\n",
    "        if encoder:\n",
    "            learn.save_encoder(f\"{model_name}_enc_{i}\")\n",
    "        print(f\"saved {model_name}_{i}\")\n",
    "        learn.recorder.plot_losses()\n",
    "        learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpCyhTE638k_"
   },
   "source": [
    "# Cleaning and Creating databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZWnflfoLvxB"
   },
   "outputs": [],
   "source": [
    "big_unstruct_id = \"1TcUKflmq4nAV2-YopYz1VqyEG9xrVIc1\"\n",
    "get_files_from_gdrive(fname=\"big_unstruct.txt\", file_id=big_unstruct_id)\n",
    "tokenizer = Tokenizer(tok_func=SpTokenizer)\n",
    "(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    ") = load_and_clean_files()\n",
    "data = load_unstructured_hinglish_from_df(\n",
    "    train,\n",
    "    test,\n",
    "    valid,\n",
    "    final_test,\n",
    "    hinglish_unsup_high_confidence,\n",
    "    hinglish_unsup_less_confidence,\n",
    ")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUd6shKfKwWi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"clean_text\"])\n",
    "train_lm, valid_lm = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kh6mZymNPl_"
   },
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=train_lm,\n",
    "    valid_df=valid_lm,\n",
    "    text_cols=\"clean_text\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"done\")\n",
    "data_lm.save(\"clean_lm.\" + \"pkl\")\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9tM2hrsPg59"
   },
   "outputs": [],
   "source": [
    "data_cls_lm = TextLMDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=pd.concat([train, final_test, test]),\n",
    "    valid_df=valid,\n",
    "    text_cols=\"clean_text\",\n",
    "    label_cols=\"sentiment\",\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=data_lm.vocab,\n",
    ")\n",
    "print(\"done\")\n",
    "data_cls_lm.save(\"clean_cls_lm.\" + \"pkl\")\n",
    "data_cls_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XQ5dxElPrhE"
   },
   "outputs": [],
   "source": [
    "data_cls = TextClasDataBunch.from_df(\n",
    "    data_folder,\n",
    "    train_df=pd.concat([train, test]),\n",
    "    valid_df=valid,\n",
    "    text_cols=\"clean_text\",\n",
    "    label_cols=\"sentiment\",\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=data_lm.vocab,\n",
    ")\n",
    "print(\"done\")\n",
    "data_cls.save(\"clean_cls.\" + \"pkl\")\n",
    "data_cls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbRYr-g0URw1"
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqAbuiUxUUEA"
   },
   "outputs": [],
   "source": [
    "data_lm = load_data(path=data_folder, file=\"clean_lm.pkl\")\n",
    "data_cls_lm = load_data(path=data_folder, file=\"clean_cls_lm.pkl\")\n",
    "data_cls = load_data(path=data_folder, file=\"clean_cls.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QltqXYyPUqyJ"
   },
   "source": [
    "# Pre-Training LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xPFw1fIUunS"
   },
   "outputs": [],
   "source": [
    "awd_lstm_lm_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.25,\n",
    "    hidden_p=0.1,\n",
    "    input_p=0.2,\n",
    "    embed_p=0.02,\n",
    "    weight_p=0.15,\n",
    "    tie_weights=True,\n",
    "    out_bias=True,\n",
    ")\n",
    "learn = language_model_learner(\n",
    "    data_lm,\n",
    "    arch=AWD_LSTM,\n",
    "    config=awd_lstm_lm_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy, Perplexity()],\n",
    "    pretrained=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWyBW5iQU0VH"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"language_model\",\n",
    "    lr=2e-03,\n",
    "    loops=5,\n",
    "    num_epocs_per_loop=2,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-aSOEu8WSx1"
   },
   "source": [
    "# Training LM Encoder with Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ey0_euRsWXLz"
   },
   "outputs": [],
   "source": [
    "awd_lstm_lm_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.25,\n",
    "    hidden_p=0.1,\n",
    "    input_p=0.2,\n",
    "    embed_p=0.02,\n",
    "    weight_p=0.15,\n",
    "    tie_weights=True,\n",
    "    out_bias=True,\n",
    ")\n",
    "learn = language_model_learner(\n",
    "    data_cls_lm, arch=AWD_LSTM, config=awd_lstm_lm_config, pretrained=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bM2NdLQIWhjX"
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "learn.load(f\"language_model_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nj77SJXeWiVm"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"language_model_class\",\n",
    "    lr=2e-03,\n",
    "    loops=10,\n",
    "    num_epocs_per_loop=10,\n",
    "    encoder=True,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVJaIdIgWymF"
   },
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOMyk_4SW0PA"
   },
   "outputs": [],
   "source": [
    "awd_lstm_clas_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.4,\n",
    "    hidden_p=0.2,\n",
    "    input_p=0.6,\n",
    "    embed_p=0.1,\n",
    "    weight_p=0.5,\n",
    ")\n",
    "learn = text_classifier_learner(\n",
    "    data_cls,\n",
    "    AWD_LSTM,\n",
    "    config=awd_lstm_clas_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy],\n",
    "    pretrained=False,\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0VCuYg1W_q-"
   },
   "outputs": [],
   "source": [
    "i = 9\n",
    "learn.load_encoder(f\"language_model_class_enc_{i}\")\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cw-wxUgmZoRm"
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    learn=learn,\n",
    "    model_name=\"class_model\",\n",
    "    lr=2e-03,\n",
    "    loops=10,\n",
    "    num_epocs_per_loop=10,\n",
    "    encoder=False,\n",
    "    test=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8ukBurzZswK"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "qYWMzJ1yZuyW",
    "outputId": "cfd1aaa2-fae2-4890-dd91-2fedb256b342"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [00:00<00:00, 426556.26it/s]\n",
      "100%|██████████| 2999/2999 [00:00<00:00, 410452.19it/s]\n",
      "100%|██████████| 2999/2999 [00:00<00:00, 312880.08it/s]\n",
      "100%|██████████| 2999/2999 [00:00<00:00, 252889.38it/s]\n",
      "100%|██████████| 2999/2999 [00:00<00:00, 221964.31it/s]\n",
      "100%|██████████| 2999/2999 [00:01<00:00, 2137.04it/s]\n"
     ]
    }
   ],
   "source": [
    "data_cls = load_data(path=data_folder, file=\"clean_cls.pkl\")\n",
    "final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
    "final_test = clean(final_test, \"text\")\n",
    "awd_lstm_clas_config = dict(\n",
    "    emb_sz=400,\n",
    "    n_hid=1150,\n",
    "    n_layers=3,\n",
    "    pad_token=1,\n",
    "    qrnn=True,\n",
    "    output_p=0.4,\n",
    "    hidden_p=0.2,\n",
    "    input_p=0.6,\n",
    "    embed_p=0.1,\n",
    "    weight_p=0.5,\n",
    ")\n",
    "learn = text_classifier_learner(\n",
    "    data_cls,\n",
    "    AWD_LSTM,\n",
    "    config=awd_lstm_clas_config,\n",
    "    drop_mult=0.5,\n",
    "    metrics=[accuracy],\n",
    "    pretrained=False,\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "79Eeo_EDZ8kH",
    "outputId": "42275ad6-fa2e-46b2-be37-dfa2d9e6eb62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [00:54<00:00, 55.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20803</td>\n",
       "      <td>@ 454dkhan @ Heisunberg _ Agr kse ko itni impo...</td>\n",
       "      <td>dkhan heisunberg agr kse ko itni importantc ch...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20187</td>\n",
       "      <td>logon ko alloo pyaz tomator me toh allah pak k...</td>\n",
       "      <td>logon ko alloo pyaz tomat toh allah pak ka naa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20953</td>\n",
       "      <td>@ LambaAlka Wafadaar bane rahane ka nayab tari...</td>\n",
       "      <td>lambaalka wafadaar bane rahan ka nayab tarika ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13777</td>\n",
       "      <td>@ varnishant @ narendramodi Chup bhosdike . He...</td>\n",
       "      <td>varnish narendramodi chup bhosdik exoner charg...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20990</td>\n",
       "      <td>RT @ HardeepSPuri à¨¦à¨¾à¨¤à¨¾ à¨¸à©‹ à¨¸à¨¾à¨...</td>\n",
       "      <td>hardeepspuri à¨¦à¨¾à¨¤à¨¾ à¨¸à©‹ à¨¸à¨¾à¨²à¨¾à...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid  ... predicted\n",
       "0  20803  ...   neutral\n",
       "1  20187  ...  positive\n",
       "2  20953  ...  positive\n",
       "3  13777  ...  negative\n",
       "4  20990  ...  positive\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 9\n",
    "learn.load(f\"test_class_model_{i}\")\n",
    "final_test[\"predicted\"] = final_test[\"clean_text\"].progress_apply(\n",
    "    lambda x: str(learn.predict(x)[0])\n",
    ")\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDD68HtBaFUu"
   },
   "outputs": [],
   "source": [
    "final_test.to_csv(data_folder / \"answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w31USaHfaQRP"
   },
   "outputs": [],
   "source": [
    "with open(data_folder / \"answer.txt\", \"w\") as f:\n",
    "    f.write(\"Uid, Sentiment\\n\")\n",
    "\n",
    "with open(data_folder / \"answer.txt\", \"a\") as f:\n",
    "    for i in range(len(final_test[\"predicted\"].tolist())):\n",
    "        f.write(f\"{final_test.loc[i]['uid']},{final_test.loc[i]['predicted']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvOPJX6WaiJ8"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(data_folder / \"answer.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9Z30bsdfCOu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPoKoSIFHe22NDtKDowYFA9",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1sopRnF1PV5uU6dOUS7sgL8qMuM3qW_Mt",
   "name": "utils.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
