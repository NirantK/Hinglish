{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1sopRnF1PV5uU6dOUS7sgL8qMuM3qW_Mt",
      "authorship_tag": "ABX9TyPoKoSIFHe22NDtKDowYFA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirantK/Hinglish/blob/utils/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoS-0gbuT0Hy",
        "colab_type": "text"
      },
      "source": [
        "# Installs \n",
        "Restart the runtime after all the installs are done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HNgydmSHL4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install gdown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVjhbwFHVTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install jsonlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhtFW2aOHey7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install ninja\n",
        "# !pip install --upgrade --force-reinstall fastai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_IDGQtnHhKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install cleantext\n",
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E32VCaB-y1J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tqdm --upgrade --force"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmMXuMF0T7AY",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Loading and Training Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n317G3WgUDhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *\n",
        "from datetime import datetime\n",
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "import cleantext\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import gdown\n",
        "tqdm.pandas()\n",
        "data_folder = Path('drive/My Drive/Hinglish/big')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUVOtz8bIh0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_files_from_gdrive(fname: str, file_id:str) -> None:\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(url, fname, quiet=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2nxVwrFJcnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(str(data_folder/\"hinglish_sp.model\"))\n",
        "\n",
        "class SpTokenizer(BaseTokenizer):\n",
        "    def __init__(self, lang: str, vocab_size: int = 8000):\n",
        "        self.lang = lang\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(data_folder/\"hinglish_sp.model\"))\n",
        "        self.vocab = Vocab([self.sp.IdToPiece(int(i)) for i in range(self.vocab_size)])\n",
        "\n",
        "    def tokenizer(self, t: str) -> List[str]:\n",
        "        return self.sp.EncodeAsPieces(t)\n",
        "\n",
        "    def detokenizer(self, t: List[str]) -> str:\n",
        "        return self.sp.DecodePieces(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LByC_kMRJvg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(df, col):\n",
        "    \"\"\"Cleaning Twitter data\n",
        "    \n",
        "    Arguments:\n",
        "        df {[pandas dataframe]} -- Dataset that needs to be cleaned\n",
        "        col {[string]} -- column in which text is present\n",
        "    \n",
        "    Returns:\n",
        "        [pandas dataframe] -- Datframe with a \"clean_text\" column\n",
        "    \"\"\"    \n",
        "    df[\"clean_text\"] = df[col]\n",
        "    df[\"clean_text\"] = (\n",
        "        (df[\"clean_text\"])\n",
        "            .progress_apply(lambda text: re.sub(r\"RT\\s@\\w+:\", \"\", text)) #Removes RTS\n",
        "            .progress_apply(lambda text: re.sub(r\"@\\w+ ?\", \"\", text)) # Replaces @ with mention\n",
        "            .progress_apply(lambda text: re.sub(r\"RT\", \"\", text)) # Replaces @ with mention\n",
        "            .progress_apply(lambda text: re.sub(r\"#\\w+ ?\", \"\", text)) # Replaces # with hastag\n",
        "            .progress_apply(lambda text: re.sub(r\"http\\S+\", \"\", text)) # Removes URL\n",
        "        )\n",
        "    df['clean_text'] = df['clean_text'].progress_apply(lambda x : cleantext.clean(x, all= True) )\n",
        "    return df\n",
        "\n",
        "toy = pd.DataFrame(['RT @meghana https://something hello'], columns=['text'])\n",
        "clean(toy, \"text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMwEOvlB2myr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_clean_files():\n",
        "    test = pd.read_json(\"drive/My Drive/Hinglish/interim/test.json\")\n",
        "    train = pd.read_json(\"drive/My Drive/Hinglish/interim/train.json\")\n",
        "    valid = pd.read_json(\"drive/My Drive/Hinglish/interim/valid.json\")\n",
        "    final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
        "    hinglish_unsup_high_confidence = pd.read_json(\"drive/My Drive/Hinglish/interim/hinglish_unsup_high_confidence.json\")\n",
        "    hinglish_unsup_less_confidence = pd.read_json(\"drive/My Drive/Hinglish/interim/hinglish_unsup_less_confidence.json\")\n",
        "    test =clean(test,'text') \n",
        "    train =clean(train,'text') \n",
        "    valid =clean(valid,'text') \n",
        "    final_test =clean(final_test,'text') \n",
        "    hinglish_unsup_high_confidence =clean(hinglish_unsup_high_confidence,0) \n",
        "    hinglish_unsup_less_confidence =clean(hinglish_unsup_less_confidence,0) \n",
        "    return train, test, valid, final_test, hinglish_unsup_high_confidence, hinglish_unsup_less_confidence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APVTHWLvKiZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_unstructured_hinglish_from_txt():   \n",
        "    data = open(\"big_unstruct.txt\", 'r').readlines()\n",
        "    data += open(data_folder/\"train.txt\", 'r').readlines()\n",
        "    data += open(data_folder/\"valid.txt\", 'r').readlines()\n",
        "    df = clean(pd.DataFrame(data, columns=['text']), 'text')\n",
        "    return df\n",
        "\n",
        "def load_unstructured_hinglish_from_df(train, test, valid, final_test, hinglish_unsup_high_confidence, hinglish_unsup_less_confidence):\n",
        "    data = list(load_unstructured_hinglish_from_txt()['clean_text'])\n",
        "    data += list(train['clean_text'])\n",
        "    data += list(test['clean_text'])\n",
        "    data += list(valid['clean_text'])\n",
        "    data += list(final_test['clean_text'])\n",
        "    data += list(hinglish_unsup_high_confidence[0])\n",
        "    data += list(hinglish_unsup_less_confidence[0])\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCwRjR5833YZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(learn, model_name:str, lr=2e-03, loops=5, num_epocs_per_loop=2, encoder=False, test=False)    \n",
        "    learn.fit_one_cycle(1, slice(lr))\n",
        "    learn.unfreeze()\n",
        "    if test:\n",
        "        learn.save(f'{model_name}_test')\n",
        "        return\n",
        "    for i in range(loops):\n",
        "        learn.unfreeze()\n",
        "        learn.fit_one_cycle(num_epocs_per_loop,slice(lr))\n",
        "        learn.save(f'{model_name}_{i}')\n",
        "        if encoder:\n",
        "            learn.save_encoder(f{model_name}_enc_{i})\n",
        "        print(f\"saved {model_name}_{i}\")\n",
        "        learn.recorder.plot_losses()\n",
        "        learn.recorder.plot_metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUhYnFAg38Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpCyhTE638k_",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning and Creating databunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWnflfoLvxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_unstruct_id = \"1TcUKflmq4nAV2-YopYz1VqyEG9xrVIc1\"\n",
        "get_files_from_gdrive(fname=\"big_unstruct.txt\", file_id=big_unstruct_id)\n",
        "tokenizer = Tokenizer(tok_func=SpTokenizer)\n",
        "train, test, valid, final_test, hinglish_unsup_high_confidence, hinglish_unsup_less_confidence = load_and_clean_files()\n",
        "data = load_unstructured_hinglish_from_df(train, test, valid, final_test, hinglish_unsup_high_confidence, hinglish_unsup_less_confidence); len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUd6shKfKwWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.DataFrame(data, columns=['clean_text'])\n",
        "train_lm, valid_lm = train_test_split(df, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kh6mZymNPl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = TextLMDataBunch.from_df(\n",
        "    data_folder,\n",
        "    train_df=train_lm,\n",
        "    valid_df = valid_lm,\n",
        "    text_cols = \"clean_text\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "print(\"done\")\n",
        "data_lm.save(\"clean_lm.\"+\"pkl\")\n",
        "data_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9tM2hrsPg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_cls_lm = TextLMDataBunch.from_df(data_folder,\n",
        "                                      train_df= pd.concat([train,final_test, test]),\n",
        "                                      valid_df= valid, \n",
        "                                      text_cols= \"clean_text\", \n",
        "                                      label_cols= \"sentiment\",\n",
        "                                      tokenizer=tokenizer,\n",
        "                                      vocab = data_lm.vocab)\n",
        "print(\"done\")\n",
        "data_cls_lm.save(\"clean_cls_lm.\"+\"pkl\")\n",
        "data_cls_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XQ5dxElPrhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_cls = TextClasDataBunch.from_df(data_folder,\n",
        "                                        train_df=pd.concat([train, test]),\n",
        "                                        valid_df= valid,\n",
        "                                        text_cols= \"clean_text\", \n",
        "                                        label_cols= \"sentiment\",\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        vocab = data_lm.vocab)\n",
        "print(\"done\")\n",
        "data_cls.save(\"clean_cls.\"+\"pkl\")\n",
        "data_cls.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbRYr-g0URw1",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqAbuiUxUUEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data(path=data_folder,file=\"clean_lm.pkl\")\n",
        "data_cls_lm = load_data(path=data_folder,file=\"clean_cls_lm.pkl\")\n",
        "data_cls = load_data(path=data_folder,file=\"clean_cls.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QltqXYyPUqyJ",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Training LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xPFw1fIUunS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "awd_lstm_lm_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=True, output_p=0.25, \n",
        "                          hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15, tie_weights=True, out_bias=True)\n",
        "learn = language_model_learner(data_lm,arch=AWD_LSTM,config=awd_lstm_lm_config, drop_mult=0.5, metrics=[accuracy, Perplexity()], pretrained=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWyBW5iQU0VH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqxbCBo0U5Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-03\n",
        "learn.fit_one_cycle(1, slice(lr))\n",
        "\n",
        "learn.unfreeze()\n",
        "# learn.save(f'test_language_model_1')\n",
        "for i in range(5):\n",
        "    learn.unfreeze()\n",
        "    learn.fit_one_cycle(2,slice(lr))\n",
        "    learn.save(f'test_language_model_{i}')\n",
        "    print(f\"saved test_language_model_{i}\")\n",
        "    learn.recorder.plot_losses()\n",
        "    learn.recorder.plot_metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-aSOEu8WSx1",
        "colab_type": "text"
      },
      "source": [
        "# Training LM Encoder with Classification Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey0_euRsWXLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "awd_lstm_lm_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=True, output_p=0.25, \n",
        "                          hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15, tie_weights=True, out_bias=True)\n",
        "learn = language_model_learner(data_cls_lm, arch=AWD_LSTM,config=awd_lstm_lm_config, pretrained=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM2NdLQIWhjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 4\n",
        "learn.load(f\"test_language_model_{i}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj77SJXeWiVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAiFGrBgWmyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-03\n",
        "learn.fit_one_cycle(1, slice(lr))\n",
        "\n",
        "learn.unfreeze()\n",
        "# learn.save_encoder(f'test_language_model_class_enc1')\n",
        "for i in range(10):\n",
        "    learn.unfreeze()\n",
        "    learn.fit_one_cycle(10,slice(lr))\n",
        "    learn.save_encoder(f'test_language_model_class_enc{i}')\n",
        "    print(f\"saved test_language_model_class_enc{i}\")\n",
        "    learn.recorder.plot_losses()\n",
        "    learn.recorder.plot_metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVJaIdIgWymF",
        "colab_type": "text"
      },
      "source": [
        "# Training Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOMyk_4SW0PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=True, output_p=0.4, \n",
        "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
        "learn = text_classifier_learner(data_cls, AWD_LSTM, config = awd_lstm_clas_config, drop_mult=0.5,metrics=[accuracy], pretrained=False).to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0VCuYg1W_q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=9\n",
        "learn.load_encoder(f\"test_language_model_class_enc{i}\")\n",
        "learn.lr_find()\n",
        "learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eCZ6OO5XY9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-03\n",
        "learn.fit_one_cycle(1, slice(lr))\n",
        "\n",
        "learn.unfreeze()\n",
        "# learn.save(f'test_class_model_1')\n",
        "for i in range(10):\n",
        "    learn.unfreeze()\n",
        "    learn.fit_one_cycle(10,slice(lr))\n",
        "    learn.save(f'test_class_model_{i}')\n",
        "    print(f\"saved test_class_model_{i}\")\n",
        "    learn.recorder.plot_losses()\n",
        "    learn.recorder.plot_metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-wxUgmZoRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ukBurzZswK",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYWMzJ1yZuyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cfd1aaa2-fae2-4890-dd91-2fedb256b342"
      },
      "source": [
        "data_cls = load_data(path=data_folder,file=\"clean_cls.pkl\")\n",
        "final_test = pd.read_json(\"drive/My Drive/Hinglish/interim/final_test.json\")\n",
        "final_test =clean(final_test,'text') \n",
        "awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=True, output_p=0.4, \n",
        "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
        "learn = text_classifier_learner(data_cls, AWD_LSTM, config = awd_lstm_clas_config, drop_mult=0.5,metrics=[accuracy], pretrained=False).to_fp16()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2999/2999 [00:00<00:00, 426556.26it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 410452.19it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 312880.08it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 252889.38it/s]\n",
            "100%|██████████| 2999/2999 [00:00<00:00, 221964.31it/s]\n",
            "100%|██████████| 2999/2999 [00:01<00:00, 2137.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Eeo_EDZ8kH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "42275ad6-fa2e-46b2-be37-dfa2d9e6eb62"
      },
      "source": [
        "i = 9\n",
        "learn.load(f'test_class_model_{i}')\n",
        "final_test['predicted'] = final_test['clean_text'].progress_apply(lambda x: str(learn.predict(x)[0]) )\n",
        "final_test.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2999/2999 [00:54<00:00, 55.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20803</td>\n",
              "      <td>@ 454dkhan @ Heisunberg _ Agr kse ko itni impo...</td>\n",
              "      <td>dkhan heisunberg agr kse ko itni importantc ch...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20187</td>\n",
              "      <td>logon ko alloo pyaz tomator me toh allah pak k...</td>\n",
              "      <td>logon ko alloo pyaz tomat toh allah pak ka naa...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20953</td>\n",
              "      <td>@ LambaAlka Wafadaar bane rahane ka nayab tari...</td>\n",
              "      <td>lambaalka wafadaar bane rahan ka nayab tarika ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13777</td>\n",
              "      <td>@ varnishant @ narendramodi Chup bhosdike . He...</td>\n",
              "      <td>varnish narendramodi chup bhosdik exoner charg...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20990</td>\n",
              "      <td>RT @ HardeepSPuri à¨¦à¨¾à¨¤à¨¾ à¨¸à©‹ à¨¸à¨¾à¨...</td>\n",
              "      <td>hardeepspuri à¨¦à¨¾à¨¤à¨¾ à¨¸à©‹ à¨¸à¨¾à¨²à¨¾à...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     uid  ... predicted\n",
              "0  20803  ...   neutral\n",
              "1  20187  ...  positive\n",
              "2  20953  ...  positive\n",
              "3  13777  ...  negative\n",
              "4  20990  ...  positive\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDD68HtBaFUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_test.to_csv(data_folder/\"answer.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w31USaHfaQRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_folder/'answer.txt', 'w') as f:\n",
        "    f.write(\"Uid, Sentiment\\n\")\n",
        "\n",
        "with open(data_folder/'answer.txt', 'a') as f:\n",
        "    for i in range(len(final_test['predicted'].tolist())):\n",
        "        f.write(f\"{final_test.loc[i]['uid']},{final_test.loc[i]['predicted']}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvOPJX6WaiJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(data_folder/'answer.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Z30bsdfCOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}